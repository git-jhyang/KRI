{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import tqdm, torch, json, pickle, os, gc, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from model.models import GraphEncoder, MoleculeEncoder, ConcatEncoder\n",
    "from utils.dataset import MoleculeDataset, collate_fn, fpolyv2_collate_fn, fpolyv1_collate_fn\n",
    "from utils.data import train_test_split, stratified_train_test_split\n",
    "from torch_geometric.nn import global_add_pool, global_max_pool, global_mean_pool\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Overwriting attribute : norm\n",
      "  Overwriting attribute : data\n",
      "  Overwriting attribute : atom_feat_name\n",
      "  Overwriting attribute : bond_feat_name\n",
      "  Overwriting attribute : mol_feat_name\n",
      "  Overwriting attribute : targets\n",
      "  Overwriting attribute : tag\n",
      "  Overwriting attribute : cache_fn\n",
      "  Setting new attribute : unique_data\n",
      "  Overwriting attribute : norm\n",
      "  Overwriting attribute : data\n",
      "  Overwriting attribute : atom_feat_name\n",
      "  Overwriting attribute : bond_feat_name\n",
      "  Overwriting attribute : mol_feat_name\n",
      "  Overwriting attribute : targets\n",
      "  Overwriting attribute : tag\n",
      "  Overwriting attribute : cache_fn\n",
      "  Setting new attribute : unique_data\n"
     ]
    }
   ],
   "source": [
    "ds1 = MoleculeDataset()\n",
    "ds2 = MoleculeDataset()\n",
    "ds1.generate_fpolyv1('../dataset/fpolymers_221123.csv', col_target='TG')\n",
    "ds2.generate_fpolyv2('../dataset/fpolymers_221123.csv', col_target='TG')\n",
    "train_data1_, test_data1 = train_test_split(ds1.data, train_ratio=0.9, seed=100)\n",
    "train_data2_, test_data2 = train_test_split(ds2.data, train_ratio=0.9, seed=100)\n",
    "train_data3_, test_data3 = stratified_train_test_split(ds2.data, stratum=[d['count'] for d in ds2.data], train_stratum=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t|  0.907 0.021|  0.998 0.001|  0.902 0.007\n",
      "valid\t|  0.561 0.190|  0.538 0.206|  0.441 0.204\n",
      "test\t|  0.611 0.066|  0.593 0.076|  0.542 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = '/home/jhyang/WORKSPACES/MODELS/fpoly/r100/finetune_n2i-02'\n",
    "tag = 'graph/dual_max_tf'\n",
    "device = 'cpu'\n",
    "results = {}\n",
    "for n in tqdm.tqdm(range(100)):\n",
    "    model_path = os.path.join(root, tag, f'c_{n:04d}')\n",
    "    if 'dual' in tag:\n",
    "        train_data, valid_data = train_test_split(train_data2_, train_ratio=0.9, seed=100+n)\n",
    "        inputs = {\n",
    "            'train': fpolyv2_collate_fn(train_data, device=device),\n",
    "            'valid': fpolyv2_collate_fn(valid_data, device=device),\n",
    "            'test': fpolyv2_collate_fn(test_data2, device=device),\n",
    "        }\n",
    "    else:\n",
    "        train_data, valid_data = train_test_split(train_data1_, train_ratio=0.9, seed=100+n)\n",
    "        inputs = {\n",
    "            'train': fpolyv1_collate_fn(train_data, device=device),\n",
    "            'valid': fpolyv1_collate_fn(valid_data, device=device),\n",
    "            'test': fpolyv1_collate_fn(test_data1, device=device),\n",
    "        }\n",
    "    if 'sing' in model_path:\n",
    "        n_head = 1\n",
    "    elif 'dual' in model_path:\n",
    "        n_head = 2\n",
    "    else:\n",
    "        n_head = 5\n",
    "    t1 = time.time()\n",
    "    sd = torch.load(os.path.join(model_path, '00200.model.torch'), map_location=device)\n",
    "    with open(os.path.join(model_path, 'param.json')) as f:\n",
    "        p = json.load(f)\n",
    "\n",
    "    if 'graph' in p['encoder_type']:\n",
    "        encoders = [GraphEncoder(**p['encoder_params']) for _ in range(n_head)]\n",
    "    elif 'mol' in p['encoder_type']:\n",
    "        encoders = [MoleculeEncoder(**p['encoder_params']) for _ in range(n_head)]\n",
    "    elif 'cat' in p['encoder_type']:\n",
    "        encoders = [ConcatEncoder(**p['encoder_params']) for _ in range(n_head)]\n",
    "\n",
    "    for i in range(n_head):\n",
    "        x = 'ABCDE'[i]\n",
    "        h = f'encoder.'\n",
    "        if 'sing' not in tag:\n",
    "            h = f'encoder_{x}.'\n",
    "        encoders[i].load_state_dict(OrderedDict({k.replace(h,''):v for k,v in sd.items() if h in k}))\n",
    "        encoders[i].to(device)\n",
    "        encoders[i].eval()\n",
    "    vectors = {}\n",
    "    with torch.no_grad():\n",
    "        for k, (feat, target, _) in inputs.items():\n",
    "            if 'dual' in model_path:\n",
    "                hs = []\n",
    "                fs = []\n",
    "                for i, x in enumerate('fc'):\n",
    "                    f = feat[f'feat_{x}']['mol_feat']\n",
    "                    h = encoders[i](**feat[f'feat_{x}'])\n",
    "                    w = feat[f'feat_{x}']['weight']\n",
    "                    b = feat[f'batch_{x}']\n",
    "                    h = torch.hstack([h, h*w])\n",
    "                    if 'add' in tag:\n",
    "                        hs.append(global_add_pool(h, b))\n",
    "                        fs.append(global_add_pool(f, b))\n",
    "                    elif 'max' in tag:\n",
    "                        hs.append(global_max_pool(h, b))\n",
    "                        fs.append(global_max_pool(f, b))\n",
    "                ms = [h.shape[0] for h in hs]\n",
    "                mx = np.max(ms)\n",
    "                p = torch.zeros_like(torch.vstack(hs))\n",
    "                h = torch.hstack([torch.vstack([h, p[:mx-m]]) for h, m in zip(hs, ms)])\n",
    "                p = torch.zeros_like(torch.vstack(fs))\n",
    "                f = torch.hstack([torch.vstack([f, p[:mx-m]]) for f, m in zip(fs, ms)])\n",
    "            else:\n",
    "                w = torch.concat([feat[f'mol_{x}']['weight'] for x in 'ABCDE'], dim=-1).to(device)\n",
    "                N, M = w.shape\n",
    "                if 'sing' in model_path:\n",
    "                    hs = [encoders[0](**feat[f'mol_{x}']) for x in 'ABCDE']\n",
    "                    fs = [feat[f'mol_{x}']['mol_feat'] for x in 'ABCDE']\n",
    "                elif 'mult' in model_path:\n",
    "                    hs = [encoders[i](**feat[f'mol_{x}']) for i,x in enumerate('ABCDE')]\n",
    "                    fs = [feat[f'mol_{x}']['mol_feat'] for i,x in enumerate('ABCDE')]\n",
    "                h = torch.concat(hs, dim=-1).view(N*M, -1)\n",
    "                f = torch.concat(fs, dim=-1).view(N*M, -1)\n",
    "                h = h * (w != 0).long().view(-1,1)\n",
    "                f = f * (w != 0).long().view(-1,1)\n",
    "                h = torch.concat([w.view(-1,1) * h, h], dim=-1)\n",
    "                f = torch.concat([w.view(-1,1) * h, h], dim=-1)\n",
    "                b = (torch.arange(N*M)/5).long().to(device)\n",
    "                if 'add' in tag:\n",
    "                    h = global_add_pool(h, b)\n",
    "                    f = global_add_pool(f, b)\n",
    "                elif 'max' in tag:\n",
    "                    h = global_max_pool(h, b)\n",
    "                    f = global_max_pool(f, b)\n",
    "            vectors[k] = (h.cpu().numpy(), f.cpu().numpy(), target.cpu().numpy())\n",
    "    results[n] = {}\n",
    "    for ds in ['train','valid','test']:\n",
    "        with open(os.path.join(model_path, f'00200.{ds}.pkl'),'rb') as f:\n",
    "            _, t, p = pickle.load(f)\n",
    "        results[n][ds] = [[t,p]]\n",
    "        \n",
    "    for i in range(2):\n",
    "        train_data = xgb.DMatrix(vectors['train'][i], vectors['train'][2])\n",
    "        valid_data = xgb.DMatrix(vectors['valid'][i], vectors['valid'][2])\n",
    "        test_data = xgb.DMatrix(vectors['test'][i], vectors['test'][2])\n",
    "        params = {'objective':'reg:squarederror', 'max_depth':8}\n",
    "        model = xgb.train(params=params, dtrain=train_data, num_boost_round=30)\n",
    "        p_train = model.predict(train_data)\n",
    "        p_valid = model.predict(valid_data)\n",
    "        p_test  = model.predict(test_data)\n",
    "        results[n]['train'].append([vectors['train'][2], p_train])\n",
    "        results[n]['valid'].append([vectors['valid'][2], p_valid])\n",
    "        results[n]['test'].append([vectors['test'][2], p_test])\n",
    "        \n",
    "infos = {'train':[], 'valid':[],'test':[]}\n",
    "for vals1 in results.values():\n",
    "    for ds, vals2 in vals1.items():\n",
    "        infos[ds].append([r2_score(t, p) for t, p in vals2])\n",
    "for ds, vals in infos.items():\n",
    "    printing = f'{ds}\\t'\n",
    "    for a, s in zip(np.mean(vals, axis=0), np.std(vals, axis=0)):\n",
    "        printing += f'| {a:6.3f} {s:5.3f}'\n",
    "    print(printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axs = plt.subplots(1,3,figsize=(10, 2.5))\n",
    "for vals, ax in zip(np.array(infos['valid']).T, axs):\n",
    "    ax.hist(vals[:10], bins=np.linspace(-1, 1.5, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09257259614350759\n",
      "0.09264016311509114\n",
      "0.09066455501723661\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root = '/home/jhyang/WORKSPACES/MODELS/fpoly/r100/encoders_2/graph/tf/'\n",
    "outs = {'train':[],'valid':[],'test':[]}\n",
    "for i in range(5):\n",
    "    for ds in ['train','valid','test']:\n",
    "        with open(os.path.join(root, f'cv_000{i}/00500.{ds}.pkl'),'rb') as f:\n",
    "            _, t, p = pickle.load(f)\n",
    "        outs[ds].append(r2_score(t,p))\n",
    "f, axs = plt.subplots(1,3,figsize=(10, 2.5))\n",
    "for vals, ax in zip(outs.values(), axs):\n",
    "    ax.hist(vals[:10], bins=np.linspace(0.0, 1.1, 50))\n",
    "    print(np.std(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mol_feat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/xgboost.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.13.24.238/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/xgboost.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(\u001b[39m*\u001b[39minputs[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mmol_feat\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.13.24.238/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/xgboost.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m valid_data \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(\u001b[39m*\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmol_feat\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.13.24.238/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/xgboost.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test_data \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(\u001b[39m*\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmol_feat\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mol_feat'"
     ]
    }
   ],
   "source": [
    "train_data = xgb.DMatrix(*inputs['train'][0]['mol_feat'])\n",
    "valid_data = xgb.DMatrix(*inputs['valid'][0]['mol_feat'])\n",
    "test_data = xgb.DMatrix(*inputs['test'][0]['mol_feat'])\n",
    "params = {'objective':'reg:squarederror', 'max_depth':10}\n",
    "model = xgb.train(params=params, dtrain=train_data, num_boost_round=20)\n",
    "p_train = model.predict(train_data)\n",
    "p_valid = model.predict(valid_data)\n",
    "p_test  = model.predict(test_data)\n",
    "for p, (_, t) in zip([p_train, p_valid, p_test], vectors.values()):\n",
    "    print(r2_score(t, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(data, c = 1):\n",
    "    feat, target, ids = fpolyv1_collate_fn(data, 'cpu')\n",
    "    \n",
    "    ws = [m['weight'] for m in feat.values()]\n",
    "    if c == 1:\n",
    "        mfs = [m['mol_feat'] for m in feat.values()]\n",
    "        ff = torch.stack([_f * _w for _f, _w in zip(mfs, ws)], dim=0).sum(dim=0).numpy()\n",
    "    elif c == 2:\n",
    "        mfs = [m['mol_feat'] for m in feat.values()]\n",
    "        ff = torch.stack([_f * _w for _f, _w in zip(mfs, ws)], dim=0).max(dim=0).values.numpy()\n",
    "    elif c == 3:\n",
    "        afs = [global_mean_pool(m['atom_feat'], m['graph_idx']) for m in feat.values()]\n",
    "        ff = torch.stack([_f * _w for _f, _w in zip(afs, ws)], dim=0).sum(dim=0).numpy()\n",
    "    elif c == 4:\n",
    "        afs = [global_mean_pool(m['atom_feat'], m['graph_idx']) for m in feat.values()]\n",
    "        ff = torch.stack([_f * _w for _f, _w in zip(afs, ws)], dim=0).max(dim=0).values.numpy()\n",
    "    return ff, target.numpy(), ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mol_avg --------------------------------------------------\n",
      "Fold: 0 :   0.999 |   0.129 /    0.88 |   31.59\n",
      "Fold: 0 :   0.999 |   0.022 /    1.12 |   28.55\n",
      "Fold: 0 :   0.992 |   0.111 /    2.87 |   26.38\n",
      "--------------------------------------------------\n",
      "Average :   0.997 0.003 |   0.087 0.046 /    1.62 0.89 |   28.84 2.14\n",
      "\n",
      "atom_avg --------------------------------------------------\n",
      "Fold: 1 :   0.000 |   0.000 /    0.00 |    0.00\n",
      "Fold: 1 :   0.985 |   0.056 /    3.82 |   28.06\n",
      "Fold: 1 :   0.982 |   0.201 /    4.29 |   25.00\n",
      "--------------------------------------------------\n",
      "Average :   0.655 0.464 |   0.086 0.085 /    2.70 1.92 |   17.69 12.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.data import CrossValidation\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "\n",
    "def obj_fnc(eta):\n",
    "    global max_depth, train_data, valid_data, m_best, best\n",
    "    params = {\n",
    "                'max_depth':max_depth,\n",
    "                'min_child_weight': 1,\n",
    "                'eta':eta,\n",
    "                # Other parameters\n",
    "                'objective':'reg:squarederror',\n",
    "                'eval_metric':'mae'\n",
    "            }\n",
    "    bst = xgb.train(params, dtrain=train_data, num_boost_round=50, evals=[(valid_data, 'valid')], early_stopping_rounds=5, verbose_eval=False)\n",
    "\n",
    "    tt = train_data.get_label()\n",
    "    vt = valid_data.get_label()\n",
    "    tp = bst.predict(train_data, iteration_range=(0, bst.best_iteration+1))\n",
    "    vp = bst.predict(valid_data, iteration_range=(0, bst.best_iteration+1))\n",
    "    train_r2, train_rmse = r2_score(tt, tp), np.sqrt(mean_squared_error(tt, tp))\n",
    "    valid_r2, valid_rmse = r2_score(vt, vp), np.sqrt(mean_squared_error(vt, vp))\n",
    "    m = np.abs(10000 * train_r2 * valid_r2 * valid_r2 / (train_rmse * valid_rmse * valid_rmse))\n",
    "    if train_r2 < 0 or valid_r2 < 0:\n",
    "        m = -m\n",
    "    if m > m_best:\n",
    "        m_best = m\n",
    "        best = {'params':params, 'r2':[train_r2, valid_r2], 'rmse':[train_rmse, valid_rmse], 'raw':[tt, tp, vt, vp]}\n",
    "    return m\n",
    "\n",
    "    \n",
    "cv = CrossValidation(n_fold=3, data=ds1.data)\n",
    "for i, feat in enumerate(['mol_avg','atom_avg']):\n",
    "    print(feat, '-'*50)\n",
    "    bests = {feat:[]}\n",
    "    vals = []\n",
    "    for j in range(3):\n",
    "        train_data, valid_data = cv[j]\n",
    "        train_x, train_y, _ = to_vector(train_data, i*2 + 1)\n",
    "        valid_x, valid_y, _ = to_vector(valid_data, i*2 + 1)\n",
    "\n",
    "        train_data = xgb.DMatrix(train_x, train_y)\n",
    "        valid_data = xgb.DMatrix(valid_y, valid_y)\n",
    "        best = {'params':[0,0], 'r2':[0,0], 'rmse':[0,0]}\n",
    "        m_best = -1e-5\n",
    "        for max_depth in range(1, 20):\n",
    "            bo = BayesianOptimization(obj_fnc, pbounds={'eta':[1e-2, 1]}, random_state=100, verbose=0)\n",
    "            bo.maximize(init_points=10, n_iter=90)\n",
    "        bests[feat].append(best)\n",
    "        tr2, vr2 = best['r2']\n",
    "        trm, vrm = best['rmse']\n",
    "        vals.append([tr2, vr2, trm, vrm])\n",
    "        print(f'Fold: {i} : {tr2:7.3f} | {vr2:7.3f} / {trm:7.2f} | {vrm:7.2f}')\n",
    "    print('-'*50)\n",
    "    vs = np.stack([np.mean(vals, axis=0), np.std(vals, axis=0)]).T.reshape(-1)\n",
    "    print('Average : {:7.3f} {:5.3f} | {:7.3f} {:5.3f} / {:7.2f} {:4.2f} | {:7.2f} {:4.2f}\\n'.format(*vs))\n",
    "    with open(f'./xgb_{feat}.pkl','wb') as f:\n",
    "        pickle.dump(bests, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'max_depth': 5,\n",
       "  'min_child_weight': 1,\n",
       "  'eta': 0.9899178523172626,\n",
       "  'objective': 'reg:squarederror',\n",
       "  'eval_metric': 'mae'},\n",
       " 'r2': [0.9927515355013128, 0.017204007501630758],\n",
       " 'rmse': [2.7201722, 26.485558],\n",
       " 'raw': [array([ 6.9150e+01,  3.6500e+01,  8.4360e+01,  4.9890e+01,  3.4730e+01,\n",
       "          9.3180e+01,  4.7900e+01,  1.0751e+02,  1.3584e+02,  5.2400e+01,\n",
       "          4.8800e+01,  6.0700e+01,  4.5620e+01,  5.2000e+01,  1.1450e+02,\n",
       "          1.0220e+02,  4.1170e+01,  5.8760e+01,  2.7100e+01,  1.0370e+02,\n",
       "          8.6640e+01,  1.0052e+02,  1.4267e+02,  2.9400e+01,  8.3450e+01,\n",
       "          6.5060e+01,  7.8870e+01,  7.7660e+01,  7.7690e+01, -2.9230e+01,\n",
       "          1.0329e+02,  3.7100e+01,  6.1470e+01,  1.0000e-01,  8.4400e+01,\n",
       "          4.8770e+01,  4.9300e+01,  6.0310e+01,  5.4350e+01,  7.8510e+01,\n",
       "          9.5410e+01,  8.3800e+01,  5.1500e+01, -1.4560e+01,  6.0080e+01,\n",
       "          7.1970e+01,  1.3362e+02,  8.6660e+01,  5.1150e+01,  9.0400e+01,\n",
       "          3.6450e+01,  8.8030e+01,  8.1090e+01,  2.5100e+01,  9.7900e+01,\n",
       "          7.1100e+01,  6.1700e+01,  4.7010e+01,  6.0320e+01,  4.8200e+01,\n",
       "          5.9800e+01,  5.6200e+01,  4.7460e+01,  5.0600e+01,  7.8680e+01,\n",
       "          1.3120e+01,  6.9580e+01,  3.6600e+01,  4.3900e+01,  8.1100e+01,\n",
       "          7.1910e+01,  5.3580e+01,  7.2600e+01,  1.0743e+02,  8.3490e+01,\n",
       "          1.0046e+02,  5.0800e+01,  5.1140e+01,  4.4640e+01,  1.1154e+02,\n",
       "          7.6480e+01,  1.0540e+02,  8.6100e+01,  3.4500e+01,  7.4930e+01,\n",
       "          1.0044e+02,  1.1569e+02,  8.9280e+01,  4.3200e+01,  5.0850e+01,\n",
       "          4.8800e+01,  1.9200e+01,  1.0153e+02,  6.2110e+01,  3.1600e+01,\n",
       "          1.1022e+02,  7.7980e+01,  5.4200e+00,  2.2040e+01,  3.7330e+01,\n",
       "          5.9140e+01,  8.3300e+01,  4.3000e+01,  6.5220e+01,  1.0050e+02,\n",
       "          4.5450e+01,  4.5650e+01, -1.4700e+01,  6.9720e+01,  6.9200e+01,\n",
       "          9.8080e+01,  6.9720e+01,  1.0227e+02, -1.2100e+00,  1.0050e+02,\n",
       "          1.3240e+02,  3.3700e+01,  1.1026e+02,  1.2860e+02,  7.6520e+01,\n",
       "          2.2100e+01,  1.2660e+02,  3.2550e+01,  6.8430e+01,  2.2510e+01,\n",
       "          8.2780e+01,  7.6100e+01,  4.8380e+01, -1.8200e+00,  1.2163e+02,\n",
       "          7.7150e+01,  2.3100e+01,  7.0660e+01,  5.4480e+01,  7.8300e+01,\n",
       "          3.7900e+01,  5.0280e+01,  1.0400e+02,  7.5440e+01,  1.4243e+02,\n",
       "          1.1378e+02,  5.7350e+01,  4.7120e+01,  6.9400e+01,  5.0350e+01,\n",
       "          6.3650e+01,  9.3800e+01,  7.7400e+01, -2.9100e+01,  9.2330e+01,\n",
       "          4.2550e+01,  6.6040e+01,  6.4990e+01,  4.2800e+01,  9.4200e+01,\n",
       "          7.0760e+01,  7.8110e+01,  9.3300e+01,  6.8730e+01,  1.0000e+01,\n",
       "          7.7200e+01,  8.0990e+01,  3.6900e+01,  5.5700e+01,  8.0040e+01,\n",
       "          7.6210e+01,  1.0740e+02,  7.9600e+01,  5.1980e+01,  3.1900e+01,\n",
       "          8.1350e+01,  8.4750e+01,  5.0690e+01,  8.1600e+01,  8.0470e+01,\n",
       "          1.1220e+02,  4.4350e+01,  3.7050e+01,  9.7570e+01,  9.9550e+01,\n",
       "          6.0400e+01,  1.6000e-01,  8.1490e+01,  6.6900e+01,  7.4790e+01,\n",
       "          8.6950e+01,  3.3400e+01,  5.4500e+01,  7.8170e+01,  9.3950e+01,\n",
       "          1.3580e+02,  9.6300e+01,  3.5700e+01,  9.9540e+01,  5.3500e+01,\n",
       "          7.7220e+01], dtype=float32),\n",
       "  array([ 67.938515  ,  36.73904   ,  86.02916   ,  47.886864  ,\n",
       "          33.6564    ,  94.61486   ,  51.196922  , 111.07725   ,\n",
       "         139.26752   ,  53.474308  ,  44.209675  ,  62.028545  ,\n",
       "          44.414215  ,  50.582752  , 115.351204  ,  97.839905  ,\n",
       "          43.075237  ,  59.59643   ,  27.47414   , 102.729164  ,\n",
       "          85.97852   , 103.07864   , 139.18604   ,  26.90679   ,\n",
       "          80.99753   ,  67.80835   ,  80.16613   ,  77.6654    ,\n",
       "          76.425385  , -25.746489  , 101.5608    ,  37.353188  ,\n",
       "          59.299816  ,   3.9232585 ,  82.424995  ,  51.06401   ,\n",
       "          51.196922  ,  57.85078   ,  56.3177    ,  78.482185  ,\n",
       "          93.40726   ,  79.46828   ,  46.596977  , -14.418034  ,\n",
       "          62.634335  ,  72.36902   , 134.10793   ,  86.85937   ,\n",
       "          44.803913  ,  91.33477   ,  34.14862   ,  83.5894    ,\n",
       "          84.350296  ,  23.685947  , 101.5608    ,  75.70684   ,\n",
       "          62.566765  ,  45.847603  ,  62.815735  ,  46.972576  ,\n",
       "          55.347897  ,  57.622765  ,  46.071564  ,  51.436535  ,\n",
       "          78.819     ,  12.533899  ,  71.67545   ,  35.994934  ,\n",
       "          41.591904  ,  81.236694  ,  63.356304  ,  55.240486  ,\n",
       "          72.55511   , 109.8899    ,  83.407524  , 100.386116  ,\n",
       "          50.587997  ,  53.418476  ,  48.92602   , 112.49165   ,\n",
       "          75.43015   , 101.95121   ,  83.48865   ,  36.694897  ,\n",
       "          72.710785  , 101.05677   , 116.16147   ,  84.53217   ,\n",
       "          45.31888   ,  55.871407  ,  47.698982  ,  20.770098  ,\n",
       "         101.5608    ,  62.438496  ,  33.828556  , 111.07725   ,\n",
       "          79.21221   ,   3.6740189 ,  22.383581  ,  39.101692  ,\n",
       "          61.761883  ,  84.53217   ,  43.108425  ,  61.457394  ,\n",
       "          99.10299   ,  41.685577  ,  45.48579   , -11.8195505 ,\n",
       "          70.09249   ,  67.95009   , 101.5608    ,  70.09249   ,\n",
       "         101.91318   ,  -1.9282569 ,  94.68416   , 128.46078   ,\n",
       "          39.3561    , 111.07725   , 123.83403   ,  74.331215  ,\n",
       "          24.02384   , 125.09164   ,  38.60155   ,  72.68377   ,\n",
       "          25.631693  ,  82.840675  ,  76.16843   ,  48.158173  ,\n",
       "          -1.9282569 , 116.31252   ,  72.42103   ,  25.479834  ,\n",
       "          66.25735   ,  63.356304  ,  81.20421   ,  40.933613  ,\n",
       "          50.31028   , 103.44466   ,  74.83558   , 136.66881   ,\n",
       "         114.86908   ,  50.986668  ,  45.18303   ,  69.681305  ,\n",
       "          46.633656  ,  61.69013   ,  96.54675   ,  76.475746  ,\n",
       "         -24.807524  ,  93.560455  ,  46.7955    ,  70.539955  ,\n",
       "          67.80835   ,  46.885693  , 100.138855  ,  70.28112   ,\n",
       "          78.349815  ,  91.65626   ,  71.67545   ,   9.411127  ,\n",
       "          73.72521   ,  80.16613   ,  40.228943  ,  53.21428   ,\n",
       "          80.90205   ,  72.68377   , 107.05045   ,  79.818535  ,\n",
       "          51.79378   ,  32.321377  ,  81.17934   ,  80.8904    ,\n",
       "          52.967644  ,  82.49727   ,  79.157776  , 108.08174   ,\n",
       "          39.3561    ,  36.9398    ,  94.61486   , 103.07864   ,\n",
       "          63.27482   ,   0.62629735,  83.5894    ,  66.34978   ,\n",
       "          77.496735  ,  87.376335  ,  32.321377  ,  53.884964  ,\n",
       "          80.75503   ,  94.00726   , 136.04074   ,  95.299034  ,\n",
       "          35.733597  , 104.42888   ,  52.961014  ,  76.56785   ],\n",
       "        dtype=float32),\n",
       "  array([ 71.18, 131.29,  62.43,  50.87,  48.7 ,  72.01,  58.14,  31.47,\n",
       "         100.84,  67.2 ,  37.32,  80.9 ,  67.87,  72.16,  88.26,  87.64,\n",
       "          40.58,  38.6 ,  23.2 ,  61.03,  81.4 , 117.12,  47.2 ,  68.73,\n",
       "          95.91,  94.4 ,  36.5 ,  63.33, 126.02, 107.28,  96.48, 114.  ,\n",
       "          82.92,  92.93,  92.2 ,  32.3 ,  52.7 ,  41.89,  84.1 , 103.35,\n",
       "          88.2 ,  72.08,  84.75,  44.51,  65.9 ,  87.62,  89.17,  71.44,\n",
       "          52.1 ,  89.5 ,  42.  ,  47.58,  81.37,  93.7 ,  36.43,  98.57,\n",
       "          72.19,  84.7 ,  83.8 ,  52.59,  77.68,  29.49,  91.5 , 111.3 ,\n",
       "          69.81,  40.07,  85.1 , 101.44,  73.56,  73.3 , 103.21,  96.5 ,\n",
       "          92.21,  65.58,  39.79, 112.14, 116.11,  98.88, 120.51,  30.  ,\n",
       "          45.9 ,  49.2 ,  39.14, 125.1 ,  37.1 ,  76.2 ,  65.  ,  86.7 ,\n",
       "          84.77,  -1.23, 105.9 ,  43.  ,  61.15,  79.9 ,  88.1 ,  90.51,\n",
       "          94.24,  83.2 ], dtype=float32),\n",
       "  array([75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 65.177795,\n",
       "         75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 , 75.64358 ,\n",
       "         75.64358 , 75.64358 ], dtype=float32)]}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-rmse:47.83573\n",
      "[1]\tvalid-rmse:50.29416\n",
      "[2]\tvalid-rmse:46.12107\n",
      "[3]\tvalid-rmse:40.24706\n",
      "[4]\tvalid-rmse:36.01260\n",
      "[5]\tvalid-rmse:34.98004\n",
      "[6]\tvalid-rmse:34.36175\n",
      "[7]\tvalid-rmse:35.06878\n",
      "[8]\tvalid-rmse:35.52806\n",
      "[9]\tvalid-rmse:34.34357\n",
      "[10]\tvalid-rmse:34.23130\n",
      "[11]\tvalid-rmse:33.47430\n",
      "[12]\tvalid-rmse:33.60856\n",
      "[13]\tvalid-rmse:35.56438\n",
      "[14]\tvalid-rmse:35.52018\n",
      "[15]\tvalid-rmse:36.50436\n",
      "[16]\tvalid-rmse:36.59705\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(\n",
    "    params={'max_depth':3, 'eta':0.3, 'eval_metric':'rmse'},\n",
    "    dtrain=train_data, num_boost_round=50, evals=[(valid_data, 'valid')],\n",
    "    early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "t = valid_data.get_label()\n",
    "for i in range(0,bst.best_iteration+5):\n",
    "    p = bst.predict(valid_data, iteration_range=(0, i+1))\n",
    "    r2 = r2_score(t, p)\n",
    "    rmse = np.sqrt(mean_squared_error(t, p))\n",
    "    score.append([i, r2, rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2745526305956041, 33.474304)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = bst.predict(valid_data, iteration_range=(0, bst.best_iteration+1))\n",
    "r2 = r2_score(t, p)\n",
    "rmse = np.sqrt(mean_squared_error(t, p))\n",
    "r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.153999</td>\n",
       "      <td>0.048631</td>\n",
       "      <td>53.824095</td>\n",
       "      <td>0.809539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.111155</td>\n",
       "      <td>0.130573</td>\n",
       "      <td>42.024383</td>\n",
       "      <td>1.751434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.176435</td>\n",
       "      <td>0.123820</td>\n",
       "      <td>34.806049</td>\n",
       "      <td>2.802056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.093472</td>\n",
       "      <td>0.115588</td>\n",
       "      <td>29.726211</td>\n",
       "      <td>2.939815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.007560</td>\n",
       "      <td>0.170853</td>\n",
       "      <td>26.816880</td>\n",
       "      <td>3.294074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.415956</td>\n",
       "      <td>0.261193</td>\n",
       "      <td>24.983658</td>\n",
       "      <td>3.347403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.920381</td>\n",
       "      <td>0.322861</td>\n",
       "      <td>24.133555</td>\n",
       "      <td>3.471736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.937590</td>\n",
       "      <td>0.113849</td>\n",
       "      <td>23.696519</td>\n",
       "      <td>3.583170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.651537</td>\n",
       "      <td>0.098528</td>\n",
       "      <td>23.345791</td>\n",
       "      <td>3.585359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.628855</td>\n",
       "      <td>0.161175</td>\n",
       "      <td>23.154989</td>\n",
       "      <td>3.613366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0        52.153999        0.048631       53.824095       0.809539\n",
       "1        38.111155        0.130573       42.024383       1.751434\n",
       "2        28.176435        0.123820       34.806049       2.802056\n",
       "3        21.093472        0.115588       29.726211       2.939815\n",
       "4        16.007560        0.170853       26.816880       3.294074\n",
       "5        12.415956        0.261193       24.983658       3.347403\n",
       "6         9.920381        0.322861       24.133555       3.471736\n",
       "7         7.937590        0.113849       23.696519       3.583170\n",
       "8         6.651537        0.098528       23.345791       3.585359\n",
       "9         5.628855        0.161175       23.154989       3.613366"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def to_vector(feat):\n",
    "    fs = []\n",
    "    for i, x in enumerate('fc'):\n",
    "        f = feat[f'feat_{x}']['mol_feat']\n",
    "        w = feat[f'feat_{x}']['weight']\n",
    "        b = feat[f'batch_{x}']\n",
    "        fs.append(global_add_pool(f * w, b))\n",
    "    ms = [f.shape[0] for f in fs]\n",
    "    mx = np.max(ms)\n",
    "    p = torch.zeros_like(torch.vstack(fs))\n",
    "    f = torch.hstack([torch.vstack([f, p[:mx-m]]) for f, m in zip(fs, ms)])\n",
    "    return f.cpu().numpy()\n",
    "\n",
    "train_feat, train_target, _ = fpolyv2_collate_fn(train_data3_)\n",
    "test_feat, test_target, _ = fpolyv2_collate_fn(test_data3)\n",
    "\n",
    "train_x = to_vector(train_feat)\n",
    "test_x = to_vector(test_feat)\n",
    "\n",
    "train_data = xgb.DMatrix(train_x, train_target.cpu().numpy())\n",
    "test_data = xgb.DMatrix(test_x, test_target.cpu().numpy())\n",
    "params = {'objective':'reg:squarederror', 'max_depth':5}\n",
    "model = xgb.cv(params=params, dtrain=train_data, num_boost_round=10)\n",
    "#p_train = model.predict(train_data)\n",
    "#p_valid = model.predict(valid_data)\n",
    "#p_test  = model.predict(test_data)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mol_A': {'atom_feat': tensor([[12.0110,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           ...,\n",
       "           [18.9980,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  5.1314],\n",
       "           [18.9980,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  5.1314],\n",
       "           [18.9980,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  5.1314]]),\n",
       "   'bond_feat': tensor([[1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           ...,\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.]]),\n",
       "   'bond_idx': tensor([[   0,    1,    1,  ..., 3287, 3282, 3288],\n",
       "           [   1,    0,    2,  ..., 3282, 3288, 3282]]),\n",
       "   'mol_feat': tensor([[12.1225, -5.7199, 12.1225,  ...,  1.2230,  2.2510,  0.0000],\n",
       "           [13.2606, -7.9449, 13.2606,  ...,  0.8450,  1.0110,  1.2360],\n",
       "           [11.2408, -4.4597, 11.2408,  ...,  2.5320,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [12.6840, -5.9423, 12.6840,  ...,  0.9580,  1.0490,  1.0960],\n",
       "           [12.6840, -5.9423, 12.6840,  ...,  0.9580,  1.0490,  1.0960],\n",
       "           [12.1225, -5.7199, 12.1225,  ...,  1.2230,  2.2510,  0.0000]]),\n",
       "   'graph_idx': tensor([[  0],\n",
       "           [  0],\n",
       "           [  0],\n",
       "           ...,\n",
       "           [195],\n",
       "           [195],\n",
       "           [195]]),\n",
       "   'weight': tensor([[0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.2000],\n",
       "           [0.6000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.5310],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.6000],\n",
       "           [0.6000],\n",
       "           [0.4000],\n",
       "           [0.5310],\n",
       "           [0.0177],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1902],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.3000],\n",
       "           [0.2000],\n",
       "           [0.0885],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.8000],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.6000],\n",
       "           [0.3000],\n",
       "           [0.2655],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.3000],\n",
       "           [0.5000],\n",
       "           [0.4000],\n",
       "           [0.0442],\n",
       "           [0.2000],\n",
       "           [1.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0442],\n",
       "           [0.6000],\n",
       "           [0.8000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.3000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.3540],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.8000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0177],\n",
       "           [0.2000],\n",
       "           [0.1930],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.6000],\n",
       "           [0.0088],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.2212],\n",
       "           [0.2000],\n",
       "           [0.3540],\n",
       "           [0.0442],\n",
       "           [0.3000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.4071],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0885],\n",
       "           [0.6000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.6000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4071],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.8000],\n",
       "           [0.2000],\n",
       "           [1.0000],\n",
       "           [0.6000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.1818],\n",
       "           [0.8000],\n",
       "           [0.0000],\n",
       "           [0.1902],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.3343],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.5000],\n",
       "           [0.3590],\n",
       "           [0.6000],\n",
       "           [0.1770],\n",
       "           [0.6000],\n",
       "           [0.3120],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.8000],\n",
       "           [0.2000],\n",
       "           [0.6000],\n",
       "           [0.0088],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.0885],\n",
       "           [0.4071],\n",
       "           [0.2000]])},\n",
       "  'mol_B': {'atom_feat': tensor([[12.0110,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  2.0000,  1.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           ...,\n",
       "           [12.0110,  3.0000,  1.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [18.9980,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  5.1314],\n",
       "           [18.9980,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  5.1314]]),\n",
       "   'bond_feat': tensor([[2., 2., 0., 1., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           [1., 1., 0., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.]]),\n",
       "   'bond_idx': tensor([[   0,    1,    1,  ..., 3825, 3824, 3826],\n",
       "           [   1,    0,    2,  ..., 3824, 3826, 3824]]),\n",
       "   'mol_feat': tensor([[10.6273, -0.4290, 10.6273,  ...,  1.3060,  0.6380,  1.0220],\n",
       "           [11.1814, -0.2537, 11.1814,  ...,  0.7050,  0.7460,  0.7930],\n",
       "           [11.1814, -0.2537, 11.1814,  ...,  0.7050,  0.7460,  0.7930],\n",
       "           ...,\n",
       "           [11.1814, -0.2537, 11.1814,  ...,  0.7050,  0.7460,  0.7930],\n",
       "           [11.1814, -0.2537, 11.1814,  ...,  0.7050,  0.7460,  0.7930],\n",
       "           [12.8110, -6.3854, 12.8110,  ...,  1.1680,  1.2590,  1.2870]]),\n",
       "   'graph_idx': tensor([[  0],\n",
       "           [  0],\n",
       "           [  0],\n",
       "           ...,\n",
       "           [195],\n",
       "           [195],\n",
       "           [195]]),\n",
       "   'weight': tensor([[0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.1062],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.3000],\n",
       "           [0.5000],\n",
       "           [0.2022],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.5000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0701],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.5000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.3000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.5000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.5000],\n",
       "           [0.0000],\n",
       "           [0.3000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.5000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.5000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.3000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1062],\n",
       "           [0.5000],\n",
       "           [0.1770],\n",
       "           [0.1000],\n",
       "           [0.3000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.1062],\n",
       "           [0.3000],\n",
       "           [0.1062],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.8000],\n",
       "           [1.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.3000],\n",
       "           [0.2000],\n",
       "           [0.1062],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.5000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.5000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.0909],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2022],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1181],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.5000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1140],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1220],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.0708],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1062],\n",
       "           [0.2000]])},\n",
       "  'mol_C': {'atom_feat': tensor([[12.0110,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [15.9990,  2.0000,  0.0000,  ...,  0.0000,  0.0000,  4.0108],\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           ...,\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164]]),\n",
       "   'bond_feat': tensor([[1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           [2., 2., 0., 1., 0.]]),\n",
       "   'bond_idx': tensor([[   0,    1,    1,  ..., 1391, 1390, 1392],\n",
       "           [   1,    0,    2,  ..., 1390, 1392, 1390]]),\n",
       "   'mol_feat': tensor([[10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.1898, -0.3472, 10.1898,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       "   'graph_idx': tensor([[  0],\n",
       "           [  0],\n",
       "           [  0],\n",
       "           ...,\n",
       "           [195],\n",
       "           [195],\n",
       "           [195]]),\n",
       "   'weight': tensor([[0.4000],\n",
       "           [0.4000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0531],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0531],\n",
       "           [0.7168],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.3043],\n",
       "           [0.7000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.4956],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.6547],\n",
       "           [0.8000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.3186],\n",
       "           [0.0000],\n",
       "           [1.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.5000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.6460],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.6903],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.3000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.5000],\n",
       "           [0.2301],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [1.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.5664],\n",
       "           [0.0000],\n",
       "           [0.3940],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.5752],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.3628],\n",
       "           [0.2000],\n",
       "           [0.2301],\n",
       "           [0.5398],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.1770],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.6000],\n",
       "           [0.0000],\n",
       "           [0.6460],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [1.0000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.1770],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.8000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4545],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.3043],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.2002],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.6000],\n",
       "           [0.5000],\n",
       "           [0.1930],\n",
       "           [0.0000],\n",
       "           [0.4071],\n",
       "           [0.0000],\n",
       "           [0.2070],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.7187],\n",
       "           [0.2000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.4000],\n",
       "           [0.2000],\n",
       "           [0.7257],\n",
       "           [0.4000],\n",
       "           [0.4000],\n",
       "           [0.4956],\n",
       "           [0.1770],\n",
       "           [0.4000]])},\n",
       "  'mol_D': {'atom_feat': tensor([[12.0110,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           ...,\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [15.9990,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  4.0108],\n",
       "           [15.9990,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  4.0108]]),\n",
       "   'bond_feat': tensor([[1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           ...,\n",
       "           [1., 1., 0., 1., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           [2., 2., 0., 1., 0.]]),\n",
       "   'bond_idx': tensor([[   0,    1,    1,  ..., 1163, 1162, 1164],\n",
       "           [   1,    0,    2,  ..., 1162, 1164, 1162]]),\n",
       "   'mol_feat': tensor([[ 9.5995, -0.9352,  9.5995,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 9.5995, -0.9352,  9.5995,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 9.2500, -0.9815,  9.2500,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 9.5995, -0.9352,  9.5995,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 9.5995, -0.9352,  9.5995,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 9.5995, -0.9352,  9.5995,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       "   'graph_idx': tensor([[  0],\n",
       "           [  0],\n",
       "           [  0],\n",
       "           ...,\n",
       "           [195],\n",
       "           [195],\n",
       "           [195]]),\n",
       "   'weight': tensor([[0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0500],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.0885],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1011],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1381],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1500],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0885],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0885],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.0790],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1062],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0885],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0909],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1011],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1161],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1120],\n",
       "           [0.4000],\n",
       "           [0.1062],\n",
       "           [0.1000],\n",
       "           [0.1200],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0726],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.0885],\n",
       "           [0.1000],\n",
       "           [0.0000],\n",
       "           [0.1062],\n",
       "           [0.1062],\n",
       "           [0.0000]])},\n",
       "  'mol_E': {'atom_feat': tensor([[12.0110,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           ...,\n",
       "           [12.0110,  2.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [12.0110,  2.0000,  2.0000,  ...,  0.0000,  0.0000,  3.3164],\n",
       "           [15.9990,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  4.0108]]),\n",
       "   'bond_feat': tensor([[1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [2., 2., 0., 1., 0.],\n",
       "           ...,\n",
       "           [1., 1., 0., 0., 1.],\n",
       "           [1., 1., 0., 0., 0.],\n",
       "           [1., 1., 0., 0., 0.]]),\n",
       "   'bond_idx': tensor([[   0,    1,    1,  ..., 1940, 1940, 1941],\n",
       "           [   1,    0,    2,  ..., 1939, 1941, 1940]]),\n",
       "   'mol_feat': tensor([[10.4688, -0.4546, 10.4688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.4688, -0.4546, 10.4688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [11.7284, -0.5700, 11.7284,  ...,  2.3590,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [10.4688, -0.4546, 10.4688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.4688, -0.4546, 10.4688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [10.4688, -0.4546, 10.4688,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       "   'graph_idx': tensor([[  0],\n",
       "           [  0],\n",
       "           [  0],\n",
       "           ...,\n",
       "           [195],\n",
       "           [195],\n",
       "           [195]]),\n",
       "   'weight': tensor([[0.2000],\n",
       "           [0.2000],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.3000],\n",
       "           [0.3000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2500],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.1770],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2022],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1371],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1500],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2212],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1770],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.1570],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1770],\n",
       "           [0.1000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1818],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2022],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2312],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.2220],\n",
       "           [0.0000],\n",
       "           [0.2035],\n",
       "           [0.2000],\n",
       "           [0.2390],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.1380],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.0000],\n",
       "           [0.1770],\n",
       "           [0.2000],\n",
       "           [0.2000],\n",
       "           [0.2035],\n",
       "           [0.2035],\n",
       "           [0.2000]])}},\n",
       " tensor([[ 33.7000],\n",
       "         [ 62.1100],\n",
       "         [ 94.2000],\n",
       "         [ 84.4000],\n",
       "         [ 72.0800],\n",
       "         [ 69.4000],\n",
       "         [ 36.5000],\n",
       "         [ 97.9000],\n",
       "         [ 44.5100],\n",
       "         [125.1000],\n",
       "         [-29.1000],\n",
       "         [ 51.1400],\n",
       "         [ 88.1000],\n",
       "         [ 39.1400],\n",
       "         [ 58.7600],\n",
       "         [ 60.7000],\n",
       "         [ 84.1000],\n",
       "         [ 52.1000],\n",
       "         [ 99.5500],\n",
       "         [ 44.3500],\n",
       "         [104.0000],\n",
       "         [ 71.9700],\n",
       "         [ 77.2000],\n",
       "         [ 36.4500],\n",
       "         [ 83.4500],\n",
       "         [ 68.7300],\n",
       "         [ 42.5500],\n",
       "         [ 69.5800],\n",
       "         [112.1400],\n",
       "         [ 66.0400],\n",
       "         [ 93.9500],\n",
       "         [111.3000],\n",
       "         [142.4300],\n",
       "         [102.2700],\n",
       "         [ 29.4000],\n",
       "         [ 86.1000],\n",
       "         [-14.7000],\n",
       "         [100.8400],\n",
       "         [ 81.3500],\n",
       "         [ 47.5800],\n",
       "         [ 77.2200],\n",
       "         [ 84.7700],\n",
       "         [ 94.2400],\n",
       "         [ 84.3600],\n",
       "         [ 83.8000],\n",
       "         [ 89.5000],\n",
       "         [ 93.8000],\n",
       "         [ 68.7300],\n",
       "         [ 65.2200],\n",
       "         [ 22.0400],\n",
       "         [ 41.8900],\n",
       "         [ 48.2000],\n",
       "         [ 43.9000],\n",
       "         [128.6000],\n",
       "         [ 78.8700],\n",
       "         [ 95.4100],\n",
       "         [ 77.9800],\n",
       "         [ 45.4500],\n",
       "         [ 72.0100],\n",
       "         [ 98.8800],\n",
       "         [ 39.7900],\n",
       "         [ 42.0000],\n",
       "         [ 92.3300],\n",
       "         [ 40.5800],\n",
       "         [ 67.2000],\n",
       "         [ 76.2000],\n",
       "         [ 80.9000],\n",
       "         [ 30.0000],\n",
       "         [103.2100],\n",
       "         [ -1.8200],\n",
       "         [ 37.1000],\n",
       "         [112.2000],\n",
       "         [135.8000],\n",
       "         [ 37.0500],\n",
       "         [ 77.6600],\n",
       "         [ 92.2100],\n",
       "         [105.9000],\n",
       "         [ 29.4900],\n",
       "         [ 60.3100],\n",
       "         [ 58.1400],\n",
       "         [110.2200],\n",
       "         [ -1.2300],\n",
       "         [107.4300],\n",
       "         [ 51.5000],\n",
       "         [114.5000],\n",
       "         [ 50.8700],\n",
       "         [ 84.7500],\n",
       "         [  5.4200],\n",
       "         [ 50.6000],\n",
       "         [ 93.3000],\n",
       "         [ 76.5200],\n",
       "         [ 48.8000],\n",
       "         [ 87.6400],\n",
       "         [ 81.0900],\n",
       "         [ 60.0800],\n",
       "         [126.6000],\n",
       "         [ 38.6000],\n",
       "         [ 49.8900],\n",
       "         [ 93.1800],\n",
       "         [ 78.1100],\n",
       "         [133.6200],\n",
       "         [ 74.9300],\n",
       "         [ 95.9100],\n",
       "         [ 67.8700],\n",
       "         [ 36.4300],\n",
       "         [ 97.5700],\n",
       "         [ 47.1200],\n",
       "         [ 71.9100],\n",
       "         [ 83.3000],\n",
       "         [ 45.6500],\n",
       "         [ 37.3300],\n",
       "         [103.2900],\n",
       "         [ 92.2000],\n",
       "         [ 33.4000],\n",
       "         [ 52.0000],\n",
       "         [ 81.6000],\n",
       "         [ 62.4300],\n",
       "         [ 83.8000],\n",
       "         [103.3500],\n",
       "         [ 63.3300],\n",
       "         [ 69.7200],\n",
       "         [ 85.1000],\n",
       "         [ 70.7600],\n",
       "         [ 82.9200],\n",
       "         [ 89.1700],\n",
       "         [ 19.2000],\n",
       "         [100.4400],\n",
       "         [ 65.0000],\n",
       "         [ 49.3000],\n",
       "         [ 35.7000],\n",
       "         [ 34.7300],\n",
       "         [ 72.1600],\n",
       "         [ 93.7000],\n",
       "         [ 65.5800],\n",
       "         [ 57.3500],\n",
       "         [ 51.1500],\n",
       "         [121.6300],\n",
       "         [ 60.4000],\n",
       "         [ 81.1000],\n",
       "         [ 22.1000],\n",
       "         [ 63.6500],\n",
       "         [ 47.9000],\n",
       "         [ 81.3700],\n",
       "         [ 73.3000],\n",
       "         [ 55.7000],\n",
       "         [ 79.9000],\n",
       "         [ 61.1500],\n",
       "         [ 50.8500],\n",
       "         [ 96.4800],\n",
       "         [107.4000],\n",
       "         [ 72.1900],\n",
       "         [ 69.8100],\n",
       "         [ 22.5100],\n",
       "         [ 50.2800],\n",
       "         [ 65.9000],\n",
       "         [ 45.6200],\n",
       "         [ 48.7700],\n",
       "         [ 88.2600],\n",
       "         [ 89.2800],\n",
       "         [120.5100],\n",
       "         [-14.5600],\n",
       "         [100.5000],\n",
       "         [117.1200],\n",
       "         [ 32.3000],\n",
       "         [ 61.7000],\n",
       "         [ 37.1000],\n",
       "         [ 66.9000],\n",
       "         [ 71.1800],\n",
       "         [ 54.4800],\n",
       "         [142.6700],\n",
       "         [ 47.2000],\n",
       "         [115.6900],\n",
       "         [ 79.6000],\n",
       "         [ 78.5100],\n",
       "         [ 52.7000],\n",
       "         [ 44.6400],\n",
       "         [ 76.4800],\n",
       "         [ 69.7200],\n",
       "         [ 81.4900],\n",
       "         [ 36.6000],\n",
       "         [ 94.4000],\n",
       "         [ 60.3200],\n",
       "         [ 49.2000],\n",
       "         [ 61.0300],\n",
       "         [ 76.1000],\n",
       "         [105.4000],\n",
       "         [-29.2300],\n",
       "         [  0.1600],\n",
       "         [ 69.1500],\n",
       "         [ 50.8000],\n",
       "         [131.2900],\n",
       "         [ 65.0600],\n",
       "         [ 90.4000],\n",
       "         [ 86.7000],\n",
       "         [ 48.8000],\n",
       "         [ 71.4400]]),\n",
       " array(['FA-00380', 'FA-00338', 'FA-00308', 'FA-00424', 'FA-00203',\n",
       "        'FA-00371', 'FA-00432', 'FA-00300', 'FA-00129', 'FA-00309',\n",
       "        'FA-00246', 'FA-00155', 'FA-00353', 'FA-00316', 'FA-00198',\n",
       "        'FA-00346', 'FA-00399', 'FA-00395', 'FA-00189', 'FA-00372',\n",
       "        'FA-00363', 'FA-00463', 'FA-00401', 'FA-00138', 'FA-00247',\n",
       "        'FA-00148', 'FA-00484', 'FA-00213', 'FA-00330', 'FA-00474',\n",
       "        'FA-00173', 'FA-00191', 'FA-00277', 'FA-00460', 'FA-00422',\n",
       "        'FA-00400', 'FA-00389', 'FA-00166', 'FA-00317', 'FA-00140',\n",
       "        'FA-00160', 'FA-00472', 'FA-00456', 'FA-00464', 'FA-00450',\n",
       "        'FA-00234', 'FA-00303', 'FA-00335', 'FA-00465', 'FA-00448',\n",
       "        'FA-00197', 'FA-00341', 'FA-00428', 'FA-00310', 'FA-00479',\n",
       "        'FA-00195', 'FA-00157', 'FA-00128', 'FA-00248', 'FA-00466',\n",
       "        'FA-00319', 'FA-00447', 'FA-00223', 'FA-00418', 'FA-00294',\n",
       "        'FA-00124', 'FA-00375', 'FA-00398', 'FA-00459', 'FA-00387',\n",
       "        'FA-00419', 'FA-00485', 'FA-00180', 'FA-00228', 'FA-00176',\n",
       "        'FA-00462', 'FA-00329', 'FA-00141', 'FA-00345', 'FA-00131',\n",
       "        'FA-00196', 'FA-00388', 'FA-00240', 'FA-00354', 'FA-00306',\n",
       "        'FA-00315', 'FA-00172', 'FA-00311', 'FA-00226', 'FA-00379',\n",
       "        'FA-00113', 'FA-00343', 'FA-00156', 'FA-00153', 'FA-00344',\n",
       "        'FA-00237', 'FA-00439', 'FA-00367', 'FA-00323', 'FA-00187',\n",
       "        'FA-00276', 'FA-00147', 'FA-00238', 'FA-00125', 'FA-00204',\n",
       "        'FA-00324', 'FA-00487', 'FA-00476', 'FA-00211', 'FA-00170',\n",
       "        'FA-00320', 'FA-00325', 'FA-00307', 'FA-00426', 'FA-00483',\n",
       "        'FA-00284', 'FA-00114', 'FA-00368', 'FA-00192', 'FA-00477',\n",
       "        'FA-00233', 'FA-00359', 'FA-00206', 'FA-00146', 'FA-00283',\n",
       "        'FA-00438', 'FA-00328', 'FA-00249', 'FA-00444', 'FA-00433',\n",
       "        'FA-00224', 'FA-00143', 'FA-00364', 'FA-00118', 'FA-00473',\n",
       "        'FA-00162', 'FA-00241', 'FA-00348', 'FA-00370', 'FA-00435',\n",
       "        'FA-00175', 'FA-00430', 'FA-00149', 'FA-00347', 'FA-00397',\n",
       "        'FA-00212', 'FA-00154', 'FA-00119', 'FA-00177', 'FA-00299',\n",
       "        'FA-00334', 'FA-00295', 'FA-00229', 'FA-00126', 'FA-00425',\n",
       "        'FA-00169', 'FA-00365', 'FA-00361', 'FA-00142', 'FA-00236',\n",
       "        'FA-00488', 'FA-00355', 'FA-00278', 'FA-00391', 'FA-00302',\n",
       "        'FA-00416', 'FA-00219', 'FA-00130', 'FA-00475', 'FA-00184',\n",
       "        'FA-00445', 'FA-00458', 'FA-00243', 'FA-00218', 'FA-00385',\n",
       "        'FA-00318', 'FA-00159', 'FA-00220', 'FA-00110', 'FA-00421',\n",
       "        'FA-00216', 'FA-00161', 'FA-00377', 'FA-00165', 'FA-00369',\n",
       "        'FA-00373', 'FA-00410', 'FA-00340', 'FA-00139', 'FA-00383',\n",
       "        'FA-00331', 'FA-00136', 'FA-00392', 'FA-00312', 'FA-00222',\n",
       "        'FA-00478'], dtype='<U8'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpolyv1_collate_fn(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301f9491a8f0e4bd7c70446afd62a207ad150af1893fa72a3c14d69f0f6f8076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
