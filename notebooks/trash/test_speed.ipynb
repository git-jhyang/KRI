{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Overwriting attribute : norm\n",
      "  Overwriting attribute : data\n",
      "  Overwriting attribute : atom_feat_name\n",
      "  Overwriting attribute : bond_feat_name\n",
      "  Overwriting attribute : mol_feat_name\n",
      "  Overwriting attribute : targets\n",
      "  Overwriting attribute : tag\n",
      "  Overwriting attribute : cache_fn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.dataset import MoleculeDataset, fpolyv2_collate_fn, to_tensor\n",
    "from utils.data import stratified_train_test_split, DataScaler, CrossValidation\n",
    "from utils.trainer import Trainer\n",
    "from utils.runner import run\n",
    "from utils.params import Parameters\n",
    "from model.models import DualHeadModel\n",
    "import torch, gc, os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "os.chdir('..')\n",
    "p = Parameters(fn='concat/mult_add_cg.json', default='defaults_r100_n3c02.json', root='./params_sc')\n",
    "p.tag = p.tag.replace('mult','dual')\n",
    "\n",
    "dataset = MoleculeDataset(norm=p.normalize_feature)\n",
    "dataset.generate_fpolyv2(path=p.data_path, col_target=p.target_column)\n",
    "\n",
    "data = to_tensor(dataset.data, device=p.device)\n",
    "stratum = np.array([len(d['F'])+len(d['C']) for d in data])\n",
    "if hasattr(p, 'train_ratio') and isinstance(p.train_ratio, float):\n",
    "    train_data_, test_data = stratified_train_test_split(data, stratum,\n",
    "                                train_ratio=p.train_ratio, seed=p.random_state)    \n",
    "elif hasattr(p, 'train_monomer'):\n",
    "    remove_monomer = getattr(p, 'remove_monomer')\n",
    "    train_data_, test_data = stratified_train_test_split(data, stratum,\n",
    "                                train_stratum=p.train_monomer, remove_stratum=remove_monomer)\n",
    "\n",
    "train_stratum = np.array([len(d['F'])+len(d['C']) for d in train_data_])\n",
    "scaler = DataScaler(device=p.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if p.cross_valid:\n",
    "    k_fold = CrossValidation(n_fold=p.num_fold, data=train_data_, \n",
    "                             stratum=train_stratum, seed=p.random_state)\n",
    "    p.num_repeat = p.num_fold\n",
    "\n",
    "p.graph_net_params.update({\n",
    "    'node_dim':dataset.num_atom_feat,\n",
    "    'edge_dim':dataset.num_bond_feat,\n",
    "})\n",
    "\n",
    "p.mol_net_params.update({\n",
    "    'input_dim':dataset.num_mol_feat,\n",
    "})\n",
    "\n",
    "p.decoder_params['output_dim'] = dataset.num_target\n",
    "\n",
    "encoder_params = {\n",
    "    'graph_net_params': p.graph_net_params,\n",
    "    'mol_net_params': p.mol_net_params,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if p.cross_valid:\n",
    "    train_data, valid_data = k_fold[0]\n",
    "else:\n",
    "    if hasattr(p, 'valid_ratio'):\n",
    "        train_data, valid_data = stratified_train_test_split(train_data_, train_stratum,\n",
    "                                        test_ratio=p.valid_ratio, seed=p.random_state)\n",
    "    else:\n",
    "        valid_data = test_data\n",
    "\n",
    "scaler.train(train_data, collate_fn=fpolyv2_collate_fn)\n",
    "\n",
    "scaled_train_data = scaler.scale_data(train_data)\n",
    "scaled_valid_data = scaler.scale_data(valid_data)\n",
    "scaled_test_data = scaler.scale_data(test_data)\n",
    "\n",
    "train_dl = DataLoader(scaled_train_data, batch_size=p.batch_size, shuffle=True, collate_fn=fpolyv2_collate_fn)\n",
    "valid_dl = DataLoader(scaled_valid_data, batch_size=512, collate_fn=fpolyv2_collate_fn)\n",
    "test_dl  = DataLoader(scaled_test_data, batch_size=512, collate_fn=fpolyv2_collate_fn)\n",
    "\n",
    "model = DualHeadModel(encoder_type=p.encoder_type, encoder_params=encoder_params, \n",
    "                        decoder_params=p.decoder_params, pooling=p.encoder_readout, \n",
    "                        shared=p.encoder_share)\n",
    "\n",
    "model.to(p.device)\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=p.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    for batch in train_dl:\n",
    "        feat, target, info = batch\n",
    " #       print(feat['feat_f']['mol_feat'].shape)\n",
    "        pred = model(**feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/test_speed.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.13.24.238/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/test_speed.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_smiles_f \u001b[39m=\u001b[39m [d[\u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msmiles\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m train_data_]\n",
      "\u001b[1;32m/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/test_speed.ipynb Cell 5\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.13.24.238/home/jhyang/WORKSPACES/CODES/fpoly/package/notebooks/test_speed.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_smiles_f \u001b[39m=\u001b[39m [d[\u001b[39m'\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39msmiles\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m train_data_]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "train_smiles_f = [d['F']['smiles'] for d in train_data_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0 1\n",
      "1 \t 0 1\n",
      "1 \t 0 1\n",
      "1 \t 1 0\n",
      "1 \t 1 0\n",
      "1 \t 1 0\n",
      "1 \t 1 0\n",
      "1 \t 0 1\n",
      "1 \t 0 1\n",
      "\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 0 2\n",
      "2 \t 0 2\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 1 1\n",
      "2 \t 0 2\n",
      "2 \t 0 2\n",
      "2 \t 1 1\n",
      "\n",
      "3 \t 1 2\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "3 \t 0 3\n",
      "3 \t 0 3\n",
      "3 \t 1 2\n",
      "3 \t 1 2\n",
      "\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 0 4\n",
      "4 \t 0 4\n",
      "4 \t 0 4\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 0 4\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 0 4\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 0 4\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 1 3\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 2 2\n",
      "4 \t 1 3\n",
      "\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "5 \t 1 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./dataset/fpolymers_221123.csv')\n",
    "df = df[~df.TG.isna()]\n",
    "df[[f'FR_{x}' for x in 'ABCDE']] = df[[f'FR_{x}' for x in 'ABCDE']].fillna(0)\n",
    "ws = df[[f'FR_{x}' for x in 'ABCDE']]\n",
    "smiles_dict = {}\n",
    "for i in range(1,6):\n",
    "    m1 = i == np.sum(ws != 0, axis=1)\n",
    "    m2 = (ws[m1] != 0).values\n",
    "    ss = df.loc[m1, [f'SMILES_{x}' for x in 'ABCDE']]\n",
    "    mf = ss.apply(lambda x: 'F' in x)\n",
    "    for s, m in zip(ss.values, m2):\n",
    "        print(i, '\\t', np.sum(['F' in _s for _s in s[m]]), np.sum(['F' not in _s for _s in s[m]]), )\n",
    "#    smiles_dict[i] = {'f':[s for s in ss if 'F' in s], 'c':[s for s in ss if 'F' not in s]}\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mask = df[[f'SMILES_{x}' for x in 'ABCDE']].apply(lambda x: ['F' in _x for _x in x])\n",
    "w_mask = df[[f'FR_{x}' for x in 'ABCDE']].fillna(0) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = np.sum(f_mask.values & w_mask.values, axis=1)\n",
    "cc = np.sum(~f_mask.values & w_mask.values, axis=1)\n",
    "ca = np.sum(w_mask.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 1, 1, 1, 4, 4, 1, 1, 3, 2, 3, 3, 3, 2, 3, 4, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 2, 3, 3, 4, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca[cf == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('poly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301f9491a8f0e4bd7c70446afd62a207ad150af1893fa72a3c14d69f0f6f8076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
