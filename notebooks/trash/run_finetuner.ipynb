{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import DataGenerator, collate_fn, to_tensor\n",
    "from utils.data import train_test_split, train_test_split_by_smiles, DataScaler\n",
    "from utils.trainer import Trainer\n",
    "from model.modules import CATEncoder, DNN, GraphEncoder, MoleculeEncoder, AddPoolFineTuner, MaxPoolFineTuner, StackFineTuner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gc, os, torch, tqdm, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate: 100%|██████████| 57/57 [00:00<00:00, 535.87it/s]\n",
      "gather: 100%|██████████| 293/293 [00:00<00:00, 1633.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 48 61\n",
      "['FC(F)(F)C(F)(F)C(F)(F)C(F)(F)CCOC(=O)C=C', 'CCCCCCOC(=O)C(C)=C', 'C=CC(=O)OCC1CCCO1', 'C=CC(=O)OCC(C(C(F)(F)F)F)(F)F', 'CC1C2CC(C1(C)C)CC2C3CCCC(C3)OC(=O)C=C', 'CC(=C)C(=O)OCCC(F)(F)C(F)(F)C(F)(F)C(F)(F)F', 'CC(=C)C(=O)OC1CCCCC1']\n",
      "['CC(=C)C(=O)OC12CC3CC(C1)CC(C3)(C2)O', 'COC(=O)C=C', 'C=CC(=O)OCCOc1ccccc1', 'CC(=C)C(=O)OC(C)(C)C', 'FC(F)(F)C(F)(F)C(F)(F)C(F)(F)C(F)(F)C(F)(F)CCOC(=O)C=C', 'FC(F)(F)C(F)(F)C(F)(F)COC(=O)C=C', 'C=CC(=O)OC1C=CC2C1C3CCC2C3']\n"
     ]
    }
   ],
   "source": [
    "seed = 102\n",
    "model_root = '/home/jhyang/WORKSPACES/MODELS/fpoly'\n",
    "\n",
    "dg = DataGenerator(None, None, include_autocorr=True)\n",
    "dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/f-polymer-20220922.csv',\n",
    "                           pfx_frac='FR', pfx_smiles='SMILES', col_target='Target', augmentation=0)\n",
    "\n",
    "train_data_, test_data, test_smiles = train_test_split_by_smiles(np.array(dg.data), n_test=7, seed=seed)\n",
    "train_data, valid_data, valid_smiles = train_test_split_by_smiles(train_data_, n_test=7, seed=seed)\n",
    "\n",
    "train_data = to_tensor(train_data)\n",
    "valid_data = to_tensor(valid_data)\n",
    "test_data = to_tensor(test_data)\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "print(valid_smiles)\n",
    "print(test_smiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate: 100%|██████████| 58/58 [00:00<00:00, 531.14it/s]\n",
      "gather: 100%|██████████| 257/257 [00:00<00:00, 1638.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 44 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_root = '/home/jhyang/WORKSPACES/MODELS/fpoly/tg_random'\n",
    "dgtg = DataGenerator(None, None, include_autocorr=True)\n",
    "\n",
    "dgtg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/dsc.csv',\n",
    "                             pfx_frac='FR', pfx_smiles='SMILES', col_target='tg')\n",
    "train_data_, test_data, t_ = train_test_split_by_smiles(dgtg.data, seed=seed, test_smiles=test_smiles)\n",
    "train_data, valid_data, v_ = train_test_split_by_smiles(train_data_, seed=seed, test_smiles=valid_smiles)\n",
    "train_data = to_tensor(train_data)\n",
    "valid_data = to_tensor(valid_data)\n",
    "test_data = to_tensor(test_data)\n",
    "print(len(train_data), len(valid_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate: 100%|██████████| 58/58 [00:00<00:00, 524.97it/s]\n",
      "gather: 100%|██████████| 257/257 [00:00<00:00, 1959.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 52 52\n",
      "['FA-00109', 'FA-00112', 'FA-00119', 'FA-00124', 'FA-00126', 'FA-00139', 'FA-00140', 'FA-00147', 'FA-00154', 'FA-00157', 'FA-00163', 'FA-00164', 'FA-00172', 'FA-00174', 'FA-00176', 'FA-00187', 'FA-00193', 'FA-00195', 'FA-00210', 'FA-00213', 'FA-00215', 'FA-00216', 'FA-00218', 'FA-00223', 'FA-00224', 'FA-00238', 'FA-00240', 'FA-00248', 'FA-00249', 'FA-00285', 'FA-00294', 'FA-00299', 'FA-00300', 'FA-00304', 'FA-00347', 'FA-00348', 'FA-00353', 'FA-00361', 'FA-00375', 'FA-00384', 'FA-00388', 'FA-00411', 'FA-00422', 'FA-00430', 'FA-00433', 'FA-00439', 'FA-00443', 'FA-00448', 'FA-00452', 'FA-00458', 'FA-00464', 'FA-00466']\n",
      "['FA-00120', 'FA-00132', 'FA-00141', 'FA-00143', 'FA-00151', 'FA-00165', 'FA-00173', 'FA-00186', 'FA-00188', 'FA-00202', 'FA-00207', 'FA-00229', 'FA-00230', 'FA-00232', 'FA-00234', 'FA-00237', 'FA-00241', 'FA-00244', 'FA-00247', 'FA-00281', 'FA-00295', 'FA-00297', 'FA-00311', 'FA-00335', 'FA-00336', 'FA-00344', 'FA-00346', 'FA-00350', 'FA-00355', 'FA-00356', 'FA-00358', 'FA-00360', 'FA-00365', 'FA-00367', 'FA-00370', 'FA-00379', 'FA-00381', 'FA-00385', 'FA-00392', 'FA-00410', 'FA-00412', 'FA-00418', 'FA-00419', 'FA-00420', 'FA-00425', 'FA-00426', 'FA-00431', 'FA-00441', 'FA-00450', 'FA-00456', 'FA-00457', 'FA-00467']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "batch_size = 20\n",
    "model_root = f'/home/jhyang/WORKSPACES/MODELS/fpoly/tg_r{seed}'\n",
    "dg = DataGenerator(None, None, include_autocorr=True)\n",
    "dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/dsc.csv',\n",
    "                           pfx_frac='FR', pfx_smiles='SMILES', col_target='tg')\n",
    "\n",
    "train_data_, test_data = train_test_split(dg.data, train_ratio=0.8, seed=seed)\n",
    "train_data, valid_data = train_test_split(train_data_, train_ratio=0.75, seed=seed)\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "train_data = to_tensor(train_data)\n",
    "valid_data = to_tensor(valid_data)\n",
    "test_data  = to_tensor(test_data)\n",
    "\n",
    "scaler = DataScaler()\n",
    "scaler.train(train_data)\n",
    "train_scaled = scaler.scale_data(train_data)\n",
    "valid_scaled = scaler.scale_data(valid_data)\n",
    "test_scaled = scaler.scale_data(test_data)\n",
    "print(sorted([d['id'] for d in valid_data]))\n",
    "print(sorted([d['id'] for d in test_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(train_dl, valid_dl, test_dl, trainer, path, epochs=10000, early_stop=500, \n",
    "             relax_after=0, logging_interval=50):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    writer = SummaryWriter(path)\n",
    "    best_loss = 1e6\n",
    "    count = 0\n",
    "    for epoch in tqdm.tqdm(range(1,epochs+1), desc=path.replace(model_root,'')):\n",
    "        if epoch == relax_after + 1:\n",
    "            for param in trainer.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            for pg in trainer.opt.param_groups:\n",
    "                pg['lr'] = pg['lr'] * 0.2\n",
    "            \n",
    "        train_loss = trainer.train(train_dl)\n",
    "        valid_loss, vi, vs, vt, vp = trainer.test(valid_dl)\n",
    "        \n",
    "        writer.add_scalar('train/loss', train_loss, epoch)\n",
    "        writer.add_scalar('valid/loss', valid_loss, epoch)\n",
    "        writer.add_scalar('valid/R2', r2_score(vt, vp), epoch)\n",
    "        writer.add_scalar('valid/MAE', mean_absolute_error(vt, vp), epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            _, _, _, t, p = trainer.test(train_dl)\n",
    "            writer.add_scalar('train/MAE', mean_absolute_error(t, p), epoch)\n",
    "            writer.add_scalar('train/R2', r2_score(t, p), epoch)\n",
    "            _, _, _, t, p = trainer.test(test_dl)\n",
    "            writer.add_scalar('test/MAE', mean_absolute_error(t, p), epoch)\n",
    "            writer.add_scalar('test/R2', r2_score(t, p), epoch)\n",
    "\n",
    "        if epoch % logging_interval == 0:\n",
    "            trainer.model.save(path, desc=f'{epoch:05d}')\n",
    "            _, ti, ts, tt, tp = trainer.test(train_dl)\n",
    "            with open(os.path.join(path, f'{epoch:05d}.train.pkl'),'wb') as f:\n",
    "                pickle.dump([ti, tt, tp], f)\n",
    "            with open(os.path.join(path, f'{epoch:05d}.valid.pkl'),'wb') as f:\n",
    "                pickle.dump([vi, vt, vp], f)\n",
    "            _, tti, tts, ttt, ttp = trainer.test(test_dl)\n",
    "            with open(os.path.join(path, f'{epoch:05d}.test.pkl'),'wb') as f:\n",
    "                pickle.dump([tti, ttt, ttp], f)\n",
    "\n",
    "        if valid_loss > best_loss:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "            best_loss = valid_loss\n",
    "            trainer.model.save(path, desc='best')\n",
    "            _, ti, ts, tt, tp = trainer.test(train_dl)\n",
    "            with open(os.path.join(path, f'best.train.pkl'),'wb') as f:\n",
    "                pickle.dump([ti, tt, tp], f)\n",
    "            with open(os.path.join(path, f'best.valid.pkl'),'wb') as f:\n",
    "                pickle.dump([vi, vt, vp], f)\n",
    "            _, tti, tts, ttt, ttp = trainer.test(test_dl)\n",
    "            with open(os.path.join(path, f'best.test.pkl'),'wb') as f:\n",
    "                pickle.dump([tti, ttt, ttp], f)\n",
    "            with open(os.path.join(path, f'best.epoch.txt'),'w') as f:\n",
    "                f.write(f'{epoch}\\n')\n",
    "        if isinstance(early_stop, int) and count >= early_stop and epoch > relax_after:\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-pretrained models \n",
    "- model dependancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_fpoly(data):\n",
    "    feat, target, ids, smiles = collate_fn(data)\n",
    "    feat = np.hstack([\n",
    "        feat['mol_feat'].cpu().numpy(),\n",
    "        feat['weight'].cpu().numpy()])\n",
    "    n_sample, n_feature = feat.shape\n",
    "    feat = feat.reshape(n_sample//5, n_feature*5)\n",
    "    target = target.cpu().numpy()\n",
    "    return feat, target, ids, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train, target_train, ids_train, _ = process_data_fpoly(train_data)\n",
    "feat_valid, target_valid, ids_valid, _ = process_data_fpoly(valid_data)\n",
    "feat_test, target_test, ids_test , _ = process_data_fpoly(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "dmat_train = xgboost.DMatrix(feat_train, target_train)\n",
    "dmat_valid = xgboost.DMatrix(feat_valid, target_valid)\n",
    "dmat_test = xgboost.DMatrix(feat_test, target_test)\n",
    "\n",
    "progress = {}\n",
    "booster = xgboost.train(dtrain=dmat_train, params={'max_depth':8, 'subsample':1}, \n",
    "                        num_boost_round=5000, early_stopping_rounds=30, \n",
    "                        evals=[(dmat_train,'Train'),(dmat_valid,'Valid')],\n",
    "                        evals_result=progress, verbose_eval=False)\n",
    "\n",
    "dest = os.path.join(model_root, 'scratch/xgboost', )\n",
    "os.makedirs(dest, exist_ok=True)\n",
    "writer = SummaryWriter(dest)\n",
    "booster.save_model(os.path.join(dest, 'model.json'))\n",
    "with open(os.path.join(dest, 'best.epoch.txt'),'w') as f:\n",
    "    f.write(f'{booster.best_iteration}\\n')\n",
    "for d in ['train','valid','test']:\n",
    "    pred = booster.predict(eval(f'dmat_{d}'), iteration_range=(0,booster.best_iteration))\n",
    "    writer.add_scalar(f'{d}/MAE', mean_absolute_error(eval(f'target_{d}'), pred), booster.best_iteration)\n",
    "    writer.add_scalar(f'{d}/R2', r2_score(eval(f'target_{d}'), pred), booster.best_iteration)\n",
    "\n",
    "    with open(os.path.join(dest, f'best.{d}.pkl'),'wb') as f:\n",
    "        pickle.dump([eval(f'ids_{d}'), eval(f'target_{d}'), pred], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- graphnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/graph/max_cg_00:  11%|█         | 1106/10000 [00:35<04:43, 31.33it/s]\n",
      "/scratch/graph/max_cg_01:  15%|█▌        | 1534/10000 [00:48<04:28, 31.57it/s]\n",
      "/scratch/graph/add_cg_00:   9%|▉         | 879/10000 [00:27<04:47, 31.78it/s]\n",
      "/scratch/graph/add_cg_01:  11%|█         | 1084/10000 [00:33<04:36, 32.26it/s]\n",
      "/scratch/graph/max_transformer_00:  20%|██        | 2009/10000 [01:40<06:41, 19.90it/s]\n",
      "/scratch/graph/max_transformer_01:  20%|██        | 2035/10000 [01:42<06:40, 19.90it/s]\n",
      "/scratch/graph/add_transformer_00:  21%|██        | 2097/10000 [01:44<06:33, 20.08it/s]\n",
      "/scratch/graph/add_transformer_01:  12%|█▏        | 1170/10000 [00:58<07:19, 20.08it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "atom_net_params  = {\n",
    "    'n_atom_feat':None,\n",
    "    'n_bond_feat':None,\n",
    "    'graph':None,\n",
    "    'hidden_dim':64,\n",
    "    'output_dim':64,\n",
    "    'n_layer':4\n",
    "}\n",
    "\n",
    "decoder_params = {\n",
    "    'input_dim':None,\n",
    "    'hidden_dims':[256, 256],\n",
    "    'output_dim':1,\n",
    "}\n",
    "\n",
    "early_stop = 500\n",
    "\n",
    "atom_net_params['n_atom_feat'] = dg.n_atom_feat\n",
    "atom_net_params['n_bond_feat'] = dg.n_bond_feat\n",
    "\n",
    "trainers = {\n",
    "    'max': (MaxPoolFineTuner, 64 * 2), \n",
    "    'add': (AddPoolFineTuner, 64 * 2), \n",
    "#    'stack': (StackFineTuner, 64 * 3 * 10), \n",
    "}\n",
    "\n",
    "for graph in ['cg','transformer']:\n",
    "    atom_net_params['graph'] = graph\n",
    "    for trtyp, (FineTuner, decoder_input_dim) in trainers.items():\n",
    "        decoder_params['input_dim'] = decoder_input_dim\n",
    "        for n in range(2):\n",
    "            scr_path = os.path.join(model_root, 'scratch/graph', f'{trtyp}_{graph}_{n:02d}')\n",
    "            if os.path.isdir(scr_path): continue\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.random.manual_seed(seed + n)\n",
    "            torch.cuda.manual_seed(seed + n)\n",
    "            train_dl = DataLoader(dataset=train_scaled, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "            valid_dl = DataLoader(dataset=valid_scaled, batch_size=256, collate_fn=collate_fn)\n",
    "            test_dl  = DataLoader(dataset=test_scaled, batch_size=256, collate_fn=collate_fn)\n",
    "\n",
    "            encoder = GraphEncoder(atom_net_params)\n",
    "            decoder = DNN(**decoder_params)\n",
    "            model = FineTuner(encoder, decoder).cuda()\n",
    "            opt   = AdamW(model.parameters(), lr=1e-4)\n",
    "            trainer = Trainer(model, opt, scaler=scaler)\n",
    "\n",
    "            do_epoch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl, trainer=trainer, path=scr_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- molnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mol/add_a000_dnn_00:  15%|█▌        | 1531/10000 [00:37<03:24, 41.38it/s]\n",
      "/scratch/mol/add_a000_dnn_01:  24%|██▎       | 2368/10000 [00:57<03:06, 41.01it/s]\n",
      "/scratch/mol/max_a000_dnn_00:  12%|█▏        | 1249/10000 [00:30<03:37, 40.30it/s]\n",
      "/scratch/mol/max_a000_dnn_01:   9%|▉         | 917/10000 [00:22<03:47, 39.87it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "mol_net_params  = {\n",
    "    'input_dim':None,\n",
    "    'hidden_dims':[256, 256, 256],\n",
    "    'output_dim':64,\n",
    "}\n",
    "early_stop = 500\n",
    "\n",
    "trainers = {\n",
    "    'add': (AddPoolFineTuner, 64 * 2), \n",
    "    'max': (MaxPoolFineTuner, 64 * 2), \n",
    "#    'stack': (StackFineTuner, 64 * 3 * 10), \n",
    "}\n",
    "mol_net_params['input_dim'] = dg.n_mol_feat \n",
    "\n",
    "for aug in [0]:\n",
    "    for trtyp, (FineTuner, decoder_input_dim) in trainers.items():\n",
    "        for n in range(2):\n",
    "            scr_path = os.path.join(model_root, 'scratch/mol', f'{trtyp}_a{aug:03d}_dnn_{n:02d}')\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.random.manual_seed(seed + n)\n",
    "            torch.cuda.manual_seed(seed + n)\n",
    "            train_dl = DataLoader(dataset=train_scaled, batch_size=batch_size*(aug+1), shuffle=True, collate_fn=collate_fn)\n",
    "            valid_dl = DataLoader(dataset=valid_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "            test_dl  = DataLoader(dataset=test_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "            \n",
    "            encoder = MoleculeEncoder(mol_net_params)\n",
    "            decoder = DNN(input_dim=decoder_input_dim, hidden_dims=[256, 256], output_dim=1)\n",
    "\n",
    "            model = FineTuner(encoder, decoder).cuda()\n",
    "            opt   = AdamW(model.parameters(), lr=1e-5)\n",
    "            trainer = Trainer(model, opt, scaler)\n",
    "            \n",
    "            do_epoch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl, trainer=trainer, path=scr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cat/add_a000_cg_00:  11%|█         | 1118/10000 [00:41<05:31, 26.79it/s]\n",
      "/scratch/cat/add_a000_cg_01:  19%|█▊        | 1868/10000 [01:08<05:00, 27.08it/s]\n",
      "/scratch/cat/max_a000_cg_00:  14%|█▎        | 1354/10000 [00:50<05:24, 26.61it/s]\n",
      "/scratch/cat/max_a000_cg_01:  21%|██        | 2099/10000 [01:18<04:57, 26.57it/s]\n",
      "/scratch/cat/stack_a000_cg_00:   6%|▋         | 646/10000 [00:23<05:44, 27.14it/s]\n",
      "/scratch/cat/stack_a000_cg_01:   8%|▊         | 834/10000 [00:30<05:35, 27.32it/s]\n",
      "/scratch/cat/add_a000_transformer_00:  15%|█▌        | 1516/10000 [01:24<07:53, 17.90it/s]\n",
      "/scratch/cat/add_a000_transformer_01:   8%|▊         | 780/10000 [00:44<08:40, 17.70it/s]\n",
      "/scratch/cat/max_a000_transformer_00:   8%|▊         | 789/10000 [00:44<08:40, 17.70it/s]\n",
      "/scratch/cat/max_a000_transformer_01:  11%|█         | 1104/10000 [01:02<08:24, 17.64it/s]\n",
      "/scratch/cat/stack_a000_transformer_00:  19%|█▉        | 1883/10000 [01:44<07:29, 18.07it/s]\n",
      "/scratch/cat/stack_a000_transformer_01:  12%|█▏        | 1245/10000 [01:09<08:05, 18.02it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "early_stop = 500\n",
    "\n",
    "atom_net_params  = {\n",
    "    'n_atom_feat':None,     'n_bond_feat':None,     'graph':None,\n",
    "    'hidden_dim':64,        'output_dim':64,        'n_layer':4\n",
    "}\n",
    "\n",
    "mol_net_params  = {\n",
    "    'input_dim':None,       'hidden_dims':[256, 256, 256],      'output_dim':64,\n",
    "}\n",
    "\n",
    "trainers = {\n",
    "    'add': (AddPoolFineTuner, 64 * 3 * 2), \n",
    "    'max': (MaxPoolFineTuner, 64 * 3 * 2), \n",
    "    'stack': (StackFineTuner, 64 * 3 * 10), \n",
    "}\n",
    "\n",
    "atom_net_params['n_atom_feat'] = dg.n_atom_feat\n",
    "atom_net_params['n_bond_feat'] = dg.n_bond_feat\n",
    "mol_net_params['input_dim'] = dg.n_mol_feat \n",
    "\n",
    "for aug in [0]:\n",
    "#    dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/f-polymer-20220922.csv',\n",
    "#                               pfx_frac='FR', pfx_smiles='SMILES', col_target='Target', augmentation=aug)\n",
    "#    dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/dsc.csv',\n",
    "#                               pfx_frac='FR', pfx_smiles='SMILES', col_target='tg', augmentation=aug)\n",
    "#    train_data_, test_data, _ = train_test_split_by_smiles(np.array(dg.data), seed=seed, test_smiles=test_smiles)\n",
    "#    train_data, valid_data, _ = train_test_split_by_smiles(train_data_, seed=seed, test_smiles=valid_smiles)\n",
    "\n",
    "#    scaler = DataScaler()\n",
    "#    train_data = to_torch(train_data)\n",
    "#    valid_data = to_torch(valid_data)\n",
    "#    test_data = to_torch(test_data)\n",
    "#    scaler.train(train_data)\n",
    "#    train_scale = scaler.scale_data(train_data)\n",
    "#    valid_scale = scaler.scale_data(valid_data)\n",
    "#    test_scale = scaler.scale_data(test_data)\n",
    "\n",
    "    for graph in ['cg','transformer']:\n",
    "        atom_net_params['graph'] = graph\n",
    "        \n",
    "        for trtyp, (FineTuner, decoder_input_dim) in trainers.items():\n",
    "            if aug > 1 and trtyp in ['add','max']: continue\n",
    "            for n in range(2):\n",
    "                scr_path = os.path.join(model_root, 'scratch/cat', f'{trtyp}_a{aug:03d}_{graph}_{n:02d}')\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.random.manual_seed(seed + n)\n",
    "                torch.cuda.manual_seed(seed + n)\n",
    "                train_dl = DataLoader(dataset=train_scaled, batch_size=batch_size*(aug+1), shuffle=True, collate_fn=collate_fn)\n",
    "                valid_dl = DataLoader(dataset=valid_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "                test_dl  = DataLoader(dataset=test_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "                \n",
    "                encoder = CATEncoder(atom_net_params, mol_net_params)\n",
    "                decoder = DNN(input_dim=decoder_input_dim, hidden_dims=[256, 256], output_dim=1)\n",
    "\n",
    "                model = FineTuner(encoder, decoder).cuda()\n",
    "                opt   = AdamW(model.parameters(), lr=3e-5)\n",
    "                trainer = Trainer(model, opt, scaler)\n",
    "                \n",
    "                do_epoch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl, trainer=trainer, path=scr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SSIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/finetune/ssib/99k/add_cg_r010_a000_00:   9%|▉         | 933/10000 [00:34<05:33, 27.22it/s]\n",
      "/finetune/ssib/99k/add_cg_r300_a000_00:   8%|▊         | 759/10000 [00:24<04:56, 31.20it/s]\n",
      "/finetune/ssib/99k/add_cg_r600_a000_00:   8%|▊         | 759/10000 [00:20<04:11, 36.78it/s]\n",
      "/finetune/ssib/ALL/add_cg_r010_a000_00:   6%|▌         | 570/10000 [00:20<05:47, 27.16it/s]\n",
      "/finetune/ssib/ALL/add_cg_r300_a000_00:   7%|▋         | 654/10000 [00:20<04:52, 31.93it/s]\n",
      "/finetune/ssib/ALL/add_cg_r600_a000_00:   7%|▋         | 654/10000 [00:16<03:58, 39.18it/s]\n",
      "/finetune/ssib/99k/max_cg_r010_a000_00:   7%|▋         | 734/10000 [00:27<05:44, 26.87it/s]\n",
      "/finetune/ssib/99k/max_cg_r300_a000_00:   7%|▋         | 698/10000 [00:22<04:58, 31.17it/s]\n",
      "/finetune/ssib/99k/max_cg_r600_a000_00:   7%|▋         | 698/10000 [00:18<04:07, 37.59it/s]\n",
      "/finetune/ssib/ALL/max_cg_r010_a000_00:  17%|█▋        | 1666/10000 [01:01<05:09, 26.93it/s]\n",
      "/finetune/ssib/ALL/max_cg_r300_a000_00:   6%|▋         | 632/10000 [00:19<04:55, 31.70it/s]\n",
      "/finetune/ssib/ALL/max_cg_r600_a000_00:  13%|█▎        | 1293/10000 [00:40<04:33, 31.87it/s]\n",
      "/finetune/ssib/99k/stack_cg_r010_a000_00:   6%|▌         | 577/10000 [00:21<05:43, 27.43it/s]\n",
      "/finetune/ssib/99k/stack_cg_r300_a000_00:   5%|▌         | 530/10000 [00:15<04:39, 33.87it/s]\n",
      "/finetune/ssib/99k/stack_cg_r600_a000_00:   6%|▌         | 600/10000 [00:14<03:45, 41.66it/s]\n",
      "/finetune/ssib/ALL/stack_cg_r010_a000_00:   9%|▉         | 914/10000 [00:33<05:28, 27.68it/s]\n",
      "/finetune/ssib/ALL/stack_cg_r300_a000_00:   9%|▉         | 949/10000 [00:30<04:52, 30.93it/s]\n",
      "/finetune/ssib/ALL/stack_cg_r600_a000_00:  10%|▉         | 964/10000 [00:27<04:18, 34.92it/s]\n",
      "/finetune/ssib/99k/add_transformer_r010_a000_00:   5%|▌         | 542/10000 [00:30<08:46, 17.97it/s]\n",
      "/finetune/ssib/99k/add_transformer_r300_a000_00:   9%|▉         | 949/10000 [00:45<07:16, 20.74it/s]\n",
      "/finetune/ssib/99k/add_transformer_r600_a000_00:  11%|█▏        | 1145/10000 [00:50<06:27, 22.88it/s]\n",
      "/finetune/ssib/ALL/add_transformer_r010_a000_00:   6%|▌         | 592/10000 [00:32<08:42, 17.99it/s]\n",
      "/finetune/ssib/ALL/add_transformer_r300_a000_00:   7%|▋         | 698/10000 [00:32<07:12, 21.51it/s]\n",
      "/finetune/ssib/ALL/add_transformer_r600_a000_00:   7%|▋         | 698/10000 [00:25<05:33, 27.91it/s]\n",
      "/finetune/ssib/99k/max_transformer_r010_a000_00:   6%|▌         | 593/10000 [00:33<08:54, 17.61it/s]\n",
      "/finetune/ssib/99k/max_transformer_r300_a000_00:   5%|▌         | 531/10000 [00:22<06:47, 23.22it/s]\n",
      "/finetune/ssib/99k/max_transformer_r600_a000_00:   6%|▌         | 600/10000 [00:19<05:01, 31.19it/s]\n",
      "/finetune/ssib/ALL/max_transformer_r010_a000_00:   6%|▋         | 648/10000 [00:36<08:50, 17.62it/s]\n",
      "/finetune/ssib/ALL/max_transformer_r300_a000_00:   6%|▌         | 573/10000 [00:25<06:51, 22.92it/s]\n",
      "/finetune/ssib/ALL/max_transformer_r600_a000_00:   6%|▌         | 600/10000 [00:19<05:01, 31.17it/s]\n",
      "/finetune/ssib/99k/stack_transformer_r010_a000_00:   8%|▊         | 780/10000 [00:43<08:31, 18.02it/s]\n",
      "/finetune/ssib/99k/stack_transformer_r300_a000_00:   6%|▌         | 570/10000 [00:24<06:47, 23.17it/s]\n",
      "/finetune/ssib/99k/stack_transformer_r600_a000_00:   6%|▌         | 600/10000 [00:19<05:00, 31.28it/s]\n",
      "/finetune/ssib/ALL/stack_transformer_r010_a000_00:   7%|▋         | 675/10000 [00:37<08:31, 18.22it/s]\n",
      "/finetune/ssib/ALL/stack_transformer_r300_a000_00:   6%|▌         | 570/10000 [00:24<06:41, 23.48it/s]\n",
      "/finetune/ssib/ALL/stack_transformer_r600_a000_00:   6%|▌         | 600/10000 [00:18<04:54, 31.87it/s]\n"
     ]
    }
   ],
   "source": [
    "pt_root = '/home/jhyang/WORKSPACES/MODELS/fpoly/'\n",
    "epochs = 10000\n",
    "early_stop = 500\n",
    "\n",
    "atom_net_params  = {\n",
    "    'n_atom_feat':None,     'n_bond_feat':None,     'graph':None,\n",
    "    'hidden_dim':64,        'output_dim':64,        'n_layer':4\n",
    "}\n",
    "\n",
    "mol_net_params  = {\n",
    "    'input_dim':None,       'hidden_dims':[256, 256, 256],      'output_dim':64,\n",
    "}\n",
    "\n",
    "trainers = {\n",
    "    'add': (AddPoolFineTuner, 64 * 3 * 2), \n",
    "    'max': (MaxPoolFineTuner, 64 * 3 * 2), \n",
    "    'stack': (StackFineTuner, 64 * 3 * 10), \n",
    "}\n",
    "\n",
    "for aug in [0]:#, 10, 30,]:\n",
    "\n",
    "    atom_net_params['n_atom_feat'] = dg.n_atom_feat\n",
    "    atom_net_params['n_bond_feat'] = dg.n_bond_feat\n",
    "    mol_net_params['input_dim'] = dg.n_mol_feat \n",
    "\n",
    "    for graph in ['cg','transformer']:\n",
    "        atom_net_params['graph'] = graph\n",
    "        \n",
    "        for trtyp, (FineTuner, decoder_input_dim) in trainers.items():\n",
    "            if aug > 1 and trtyp in ['add','max']: continue\n",
    "            for amnt in ['99k','ALL']:\n",
    "                pt_path = os.path.join(pt_root, f'encoders/ssib/U_wF_{amnt}/{graph}_pt')\n",
    "                if not os.path.isdir(pt_path):\n",
    "                    continue\n",
    "                for relax_after in [10, 300, 600]:\n",
    "                    for n in range(1):\n",
    "                        ft_path = os.path.join(model_root, 'finetune/ssib', f'{amnt}', f'{trtyp}_{graph}_r{relax_after:03d}_a{aug:03d}_{n:02d}')\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.random.manual_seed(seed + n)\n",
    "                        torch.cuda.manual_seed(seed + n)\n",
    "\n",
    "                        train_dl = DataLoader(dataset=train_scaled, batch_size=batch_size*(aug+1), shuffle=True, collate_fn=collate_fn)\n",
    "                        valid_dl = DataLoader(dataset=valid_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "                        test_dl  = DataLoader(dataset=test_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "\n",
    "                        encoder = CATEncoder(atom_net_params, mol_net_params)\n",
    "                        encoder.load(pt_path, desc='best', freeze=True)\n",
    "                        decoder = DNN(input_dim=decoder_input_dim, hidden_dims=[256, 256], output_dim=1)\n",
    "\n",
    "                        model = FineTuner(encoder, decoder).cuda()\n",
    "                        opt   = AdamW(model.parameters(), lr=3e-5)\n",
    "                        trainer = Trainer(model, opt, scaler)\n",
    "\n",
    "                        do_epoch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl, trainer=trainer, \n",
    "                                 path=ft_path, relax_after=relax_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/finetune/cat/99k/add_cg_r000_a000_00:  15%|█▍        | 1452/10000 [00:54<05:18, 26.88it/s]\n",
      "/finetune/cat/99k/add_cg_r300_a000_00:  13%|█▎        | 1286/10000 [00:43<04:55, 29.47it/s]\n",
      "/finetune/cat/99k/add_cg_r600_a000_00:  23%|██▎       | 2306/10000 [01:17<04:18, 29.77it/s]\n",
      "/finetune/cat/99k/add_cg_r000_a000_01:   6%|▌         | 565/10000 [00:21<05:52, 26.75it/s]\n",
      "/finetune/cat/99k/add_cg_r300_a000_01:   5%|▌         | 532/10000 [00:15<04:41, 33.67it/s]\n",
      "/finetune/cat/99k/add_cg_r600_a000_01:   6%|▌         | 600/10000 [00:14<03:52, 40.50it/s]\n",
      "/finetune/cat/ALL/add_cg_r000_a000_00:  22%|██▏       | 2240/10000 [01:22<04:47, 27.04it/s]\n",
      "/finetune/cat/ALL/add_cg_r300_a000_00:  20%|██        | 2040/10000 [01:11<04:39, 28.53it/s]\n",
      "/finetune/cat/ALL/add_cg_r600_a000_00:   8%|▊         | 805/10000 [00:22<04:14, 36.19it/s]\n",
      "/finetune/cat/ALL/add_cg_r000_a000_01:  12%|█▏        | 1235/10000 [00:45<05:24, 26.98it/s]\n",
      "/finetune/cat/ALL/add_cg_r300_a000_01:  23%|██▎       | 2287/10000 [01:20<04:30, 28.55it/s]\n",
      "/finetune/cat/ALL/add_cg_r600_a000_01:   9%|▊         | 857/10000 [00:23<04:15, 35.77it/s]\n",
      "/finetune/cat/99k/max_cg_r000_a000_00:  21%|██        | 2121/10000 [01:19<04:54, 26.71it/s]\n",
      "/finetune/cat/99k/max_cg_r300_a000_00:  15%|█▌        | 1519/10000 [00:53<04:58, 28.40it/s]\n",
      "/finetune/cat/99k/max_cg_r600_a000_00:  11%|█         | 1070/10000 [00:32<04:33, 32.67it/s]\n",
      "/finetune/cat/99k/max_cg_r000_a000_01:  12%|█▏        | 1217/10000 [00:46<05:32, 26.43it/s]\n",
      "/finetune/cat/99k/max_cg_r300_a000_01:  20%|█▉        | 1994/10000 [01:10<04:44, 28.13it/s]\n",
      "/finetune/cat/99k/max_cg_r600_a000_01:   9%|▉         | 886/10000 [00:25<04:27, 34.12it/s]\n",
      "/finetune/cat/ALL/max_cg_r000_a000_00:  12%|█▏        | 1249/10000 [00:47<05:29, 26.54it/s]\n",
      "/finetune/cat/ALL/max_cg_r300_a000_00:  12%|█▏        | 1163/10000 [00:39<05:02, 29.22it/s]\n",
      "/finetune/cat/ALL/max_cg_r600_a000_00:  14%|█▍        | 1390/10000 [00:44<04:34, 31.36it/s]\n",
      "/finetune/cat/ALL/max_cg_r000_a000_01:  27%|██▋       | 2725/10000 [01:41<04:31, 26.75it/s]\n",
      "/finetune/cat/ALL/max_cg_r300_a000_01:   8%|▊         | 804/10000 [00:26<05:00, 30.65it/s]\n",
      "/finetune/cat/ALL/max_cg_r600_a000_01:   8%|▊         | 831/10000 [00:23<04:17, 35.59it/s]\n",
      "/finetune/cat/99k/stack_cg_r000_a000_00:  15%|█▍        | 1487/10000 [00:54<05:11, 27.33it/s]\n",
      "/finetune/cat/99k/stack_cg_r300_a000_00:  10%|█         | 1000/10000 [00:32<04:53, 30.64it/s]\n",
      "/finetune/cat/99k/stack_cg_r600_a000_00:  11%|█         | 1090/10000 [00:31<04:21, 34.12it/s]\n",
      "/finetune/cat/99k/stack_cg_r000_a000_01:   6%|▌         | 592/10000 [00:21<05:45, 27.27it/s]\n",
      "/finetune/cat/99k/stack_cg_r300_a000_01:   6%|▌         | 622/10000 [00:18<04:43, 33.10it/s]\n",
      "/finetune/cat/99k/stack_cg_r600_a000_01:   9%|▉         | 900/10000 [00:25<04:15, 35.61it/s]\n",
      "/finetune/cat/ALL/stack_cg_r000_a000_00:   7%|▋         | 748/10000 [00:27<05:37, 27.41it/s]\n",
      "/finetune/cat/ALL/stack_cg_r300_a000_00:   9%|▉         | 883/10000 [00:28<04:51, 31.32it/s]\n",
      "/finetune/cat/ALL/stack_cg_r600_a000_00:   9%|▉         | 883/10000 [00:24<04:12, 36.17it/s]\n",
      "/finetune/cat/ALL/stack_cg_r000_a000_01:  11%|█         | 1083/10000 [00:39<05:23, 27.58it/s]\n",
      "/finetune/cat/ALL/stack_cg_r300_a000_01:   6%|▌         | 580/10000 [00:17<04:42, 33.36it/s]\n",
      "/finetune/cat/ALL/stack_cg_r600_a000_01:   6%|▌         | 600/10000 [00:14<03:45, 41.66it/s]\n",
      "/finetune/cat/99k/add_transformer_r000_a000_00:   7%|▋         | 729/10000 [00:40<08:39, 17.85it/s]\n",
      "/finetune/cat/99k/add_transformer_r300_a000_00:   5%|▌         | 535/10000 [00:22<06:41, 23.60it/s]\n",
      "/finetune/cat/99k/add_transformer_r600_a000_00:   6%|▌         | 600/10000 [00:19<04:59, 31.43it/s]\n",
      "/finetune/cat/99k/add_transformer_r000_a000_01:  29%|██▉       | 2933/10000 [02:44<06:35, 17.86it/s]\n",
      "/finetune/cat/99k/add_transformer_r300_a000_01:  18%|█▊        | 1810/10000 [01:34<07:05, 19.23it/s]\n",
      "/finetune/cat/99k/add_transformer_r600_a000_01:  12%|█▏        | 1156/10000 [00:50<06:23, 23.08it/s]\n",
      "/finetune/cat/ALL/add_transformer_r000_a000_00:   7%|▋         | 727/10000 [00:40<08:39, 17.85it/s]\n",
      "/finetune/cat/ALL/add_transformer_r300_a000_00:   8%|▊         | 799/10000 [00:37<07:07, 21.50it/s]\n",
      "/finetune/cat/ALL/add_transformer_r600_a000_00:  10%|▉         | 961/10000 [00:38<06:06, 24.65it/s]\n",
      "/finetune/cat/ALL/add_transformer_r000_a000_01:  18%|█▊        | 1809/10000 [01:41<07:41, 17.74it/s]\n",
      "/finetune/cat/ALL/add_transformer_r300_a000_01:  12%|█▏        | 1187/10000 [00:58<07:16, 20.18it/s]\n",
      "/finetune/cat/ALL/add_transformer_r600_a000_01:  10%|▉         | 956/10000 [00:38<06:08, 24.55it/s]\n",
      "/finetune/cat/99k/max_transformer_r000_a000_00:  11%|█         | 1083/10000 [01:01<08:26, 17.62it/s]\n",
      "/finetune/cat/99k/max_transformer_r300_a000_00:  14%|█▍        | 1435/10000 [01:13<07:18, 19.54it/s]\n",
      "/finetune/cat/99k/max_transformer_r600_a000_00:  10%|█         | 1017/10000 [00:42<06:19, 23.66it/s]\n",
      "/finetune/cat/99k/max_transformer_r000_a000_01:  13%|█▎        | 1262/10000 [01:12<08:21, 17.43it/s]\n",
      "/finetune/cat/99k/max_transformer_r300_a000_01:  16%|█▌        | 1588/10000 [01:23<07:24, 18.92it/s]\n",
      "/finetune/cat/99k/max_transformer_r600_a000_01:  13%|█▎        | 1297/10000 [00:59<06:36, 21.94it/s]\n",
      "/finetune/cat/ALL/max_transformer_r000_a000_00:  12%|█▏        | 1191/10000 [01:08<08:23, 17.48it/s]\n",
      "/finetune/cat/ALL/max_transformer_r300_a000_00:  13%|█▎        | 1282/10000 [01:05<07:23, 19.68it/s]\n",
      "/finetune/cat/ALL/max_transformer_r600_a000_00:  14%|█▍        | 1407/10000 [01:05<06:37, 21.64it/s]\n",
      "/finetune/cat/ALL/max_transformer_r000_a000_01:  34%|███▎      | 3362/10000 [03:08<06:12, 17.83it/s]\n",
      "/finetune/cat/ALL/max_transformer_r300_a000_01:   9%|▊         | 863/10000 [00:41<07:16, 20.91it/s]\n",
      "/finetune/cat/ALL/max_transformer_r600_a000_01:   9%|▉         | 945/10000 [00:38<06:06, 24.69it/s]\n",
      "/finetune/cat/99k/stack_transformer_r000_a000_00:  13%|█▎        | 1314/10000 [01:12<08:02, 18.01it/s]\n",
      "/finetune/cat/99k/stack_transformer_r300_a000_00:  15%|█▌        | 1540/10000 [01:18<07:13, 19.54it/s]\n",
      "/finetune/cat/99k/stack_transformer_r600_a000_00:  10%|█         | 1005/10000 [00:41<06:08, 24.40it/s]\n",
      "/finetune/cat/99k/stack_transformer_r000_a000_01:   8%|▊         | 758/10000 [00:42<08:38, 17.83it/s]\n",
      "/finetune/cat/99k/stack_transformer_r300_a000_01:   6%|▌         | 562/10000 [00:23<06:41, 23.50it/s]\n",
      "/finetune/cat/99k/stack_transformer_r600_a000_01:   9%|▉         | 917/10000 [00:36<06:00, 25.17it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r000_a000_00:  14%|█▍        | 1378/10000 [01:16<07:57, 18.06it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r300_a000_00:   7%|▋         | 711/10000 [00:32<06:58, 22.18it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r600_a000_00:   7%|▋         | 711/10000 [00:24<05:24, 28.59it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r000_a000_01:   8%|▊         | 784/10000 [00:43<08:35, 17.89it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r300_a000_01:  15%|█▌        | 1518/10000 [01:16<07:06, 19.88it/s]\n",
      "/finetune/cat/ALL/stack_transformer_r600_a000_01:  14%|█▎        | 1361/10000 [01:00<06:23, 22.55it/s]\n"
     ]
    }
   ],
   "source": [
    "pt_root = '/home/jhyang/WORKSPACES/MODELS/fpoly/'\n",
    "\n",
    "epochs = 10000\n",
    "early_stop = 500\n",
    "\n",
    "atom_net_params  = {\n",
    "    'n_atom_feat':None,     'n_bond_feat':None,     'graph':None,\n",
    "    'hidden_dim':64,        'output_dim':64,        'n_layer':4\n",
    "}\n",
    "\n",
    "mol_net_params  = {\n",
    "    'input_dim':None,       'hidden_dims':[256, 256, 256],      'output_dim':64,\n",
    "}\n",
    "\n",
    "trainers = {\n",
    "    'add': (AddPoolFineTuner, 64 * 3 * 2), \n",
    "    'max': (MaxPoolFineTuner, 64 * 3 * 2), \n",
    "    'stack': (StackFineTuner, 64 * 3 * 10), \n",
    "}\n",
    "\n",
    "for aug in [0]:#, 10, 30]:\n",
    "#    dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/f-polymer-20220922.csv',\n",
    "#                               pfx_frac='FR', pfx_smiles='SMILES', col_target='Target', augmentation=aug)\n",
    "#    dg.generate_fpoly_from_csv('/home/jhyang/WORKSPACES/DATA/polymers/f-polymer/dsc.csv',\n",
    "#                               pfx_frac='FR', pfx_smiles='SMILES', col_target='tg', augmentation=aug)\n",
    "#\n",
    "#    train_data_, test_data, test_smiles = train_test_split_by_smiles(np.array(dg.data), test_smiles=test_smiles)\n",
    "#    train_data, valid_data, valid_smiles = train_test_split_by_smiles(train_data_, test_smiles=valid_smiles)\n",
    "#\n",
    "#    scaler = DataScaler()\n",
    "#    train_data = to_torch(train_data)\n",
    "#    valid_data = to_torch(valid_data)\n",
    "#    test_data = to_torch(test_data)\n",
    "#    scaler.train(train_data)\n",
    "#    train_scale = scaler.scale_data(train_data)\n",
    "#    valid_scale = scaler.scale_data(valid_data)\n",
    "#    test_scale = scaler.scale_data(test_data)\n",
    "\n",
    "    atom_net_params['n_atom_feat'] = dg.n_atom_feat\n",
    "    atom_net_params['n_bond_feat'] = dg.n_bond_feat\n",
    "    mol_net_params['input_dim'] = dg.n_mol_feat \n",
    "\n",
    "    for graph in ['cg','transformer']:\n",
    "        atom_net_params['graph'] = graph\n",
    "        \n",
    "        for trtyp, (FineTuner, decoder_input_dim) in trainers.items():\n",
    "            if aug > 1 and trtyp in ['add','max']: continue\n",
    "            for amnt in ['99k','ALL']:\n",
    "                for n in range(2):\n",
    "                    pt_path = os.path.join(pt_root, f'encoders/cat/U_wF_{amnt}/{graph}_{n:02d}')\n",
    "                    if not os.path.isdir(pt_path):\n",
    "                        continue\n",
    "                    for relax_after in [0, 300, 600]:\n",
    "                        ft_path = os.path.join(model_root, 'finetune/cat', f'{amnt}', f'{trtyp}_{graph}_r{relax_after:03d}_a{aug:03d}_{n:02d}')\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.random.manual_seed(seed + n)\n",
    "                        torch.cuda.manual_seed(seed + n)\n",
    "\n",
    "                        train_dl = DataLoader(dataset=train_scaled, batch_size=batch_size*(aug+1), shuffle=True, collate_fn=collate_fn)\n",
    "                        valid_dl = DataLoader(dataset=valid_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "                        test_dl  = DataLoader(dataset=test_scaled, batch_size=2048, collate_fn=collate_fn)\n",
    "\n",
    "                        encoder = CATEncoder(atom_net_params, mol_net_params)\n",
    "                        encoder.load(pt_path, desc='best', freeze=True)\n",
    "                        decoder = DNN(input_dim=decoder_input_dim, hidden_dims=[256, 256], output_dim=1)\n",
    "\n",
    "                        model = FineTuner(encoder, decoder).cuda()\n",
    "                        opt   = AdamW(model.parameters(), lr=3e-5)\n",
    "                        trainer = Trainer(model, opt, scaler)\n",
    "\n",
    "                        do_epoch(train_dl=train_dl, valid_dl=valid_dl, test_dl=test_dl, trainer=trainer, \n",
    "                                 path=ft_path, relax_after=relax_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('poly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301f9491a8f0e4bd7c70446afd62a207ad150af1893fa72a3c14d69f0f6f8076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
